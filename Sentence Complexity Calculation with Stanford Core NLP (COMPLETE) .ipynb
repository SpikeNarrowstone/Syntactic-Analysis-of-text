{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\spike\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\spike\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\spike\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\spike\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "Done loading processors!\n",
      "---\n",
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\spike\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\spike\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\spike\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\spike\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\spike\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\spike\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import stanfordnlp\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP(r'C:\\Users\\spike\\python\\stanford-corenlp-full-2018-10-05',lang='en')\n",
    "\n",
    "xnlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "xnlp = stanfordnlp.Pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "avoidance=string.punctuation+string.whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readfile(file):\n",
    "    list_sents={}\n",
    "    name=os.path.basename(file)\n",
    "    if file[-4:]==\".txt\":\n",
    "        text=open(file,encoding='utf-8',errors='ignore')\n",
    "        avoidance=string.punctuation+string.whitespace\n",
    "        option=(\"A.\",\"B.\",\"C.\",\"D.\",\"E.\",\"F.\",\"G.\",\"B \",\"C \",\"D \",\"E \",\"F \",\"G \")\n",
    "    #print(avoidance)\n",
    "        line=''\n",
    "        for chline in text:\n",
    "            #for w in option:\n",
    "                #chline=chline.replace(w,\" \")           \n",
    "            for character in chline:\n",
    "                #if character in option and chline[chline.index(character)+1] in (\".\",\" \"):\n",
    "                    #line+=\" \"\n",
    "                if character==\"'\" or character=='\"' or character==\"’\"or character==\"“\" or character==\"”\":\n",
    "                    line+=character\n",
    "                elif character in (\"_\"):\n",
    "                    line+=\" \"\n",
    "                elif character in (\"(\",\")\",\"（\",\"）\"):\n",
    "                    line+=\" \"+character+' '\n",
    "                elif character in avoidance:\n",
    "                    line+=character\n",
    "                elif ord(character) in range(32,123):# and character not in '\\t1234567890':\n",
    "                    line+=character\n",
    "                else:\n",
    "                    line+=\"\"\n",
    "        \n",
    "        text=\"\"\n",
    "    #text=text.join(txt).replace(\"\\n\",' ')\n",
    "        text=line.replace(\"“\",'\"').replace(\"”\",'\"').replace(\"’\",\"'\").replace(\"‘\",\"'\")\n",
    "\n",
    "        return text\n",
    "    else:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(s):\n",
    "    output=[]\n",
    "    doc = xnlp(s)\n",
    "    for sent in doc.sentences:\n",
    "        for word in sent.words:\n",
    "            output.append(word.lemma)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc = xnlp(\"Barack Obama was born in Hawaii.\")\n",
    "#print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gonver(s):\n",
    "    output=[]\n",
    "    doc=xnlp(s)\n",
    "    for word in doc.sentences[0].words:\n",
    "        output.append(word.governor)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dependency(s):\n",
    "    s=nlp.dependency_parse(s)\n",
    "    output=[]\n",
    "    for n in range(1,len(s)+1):\n",
    "        for w in s:\n",
    "            if w[2]==n:\n",
    "                output.append(w)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#nlp.annotate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanz(s): #分析一个句子中的各个成分包括单词、lemma、词性、依存关系、序列\n",
    "    words=nlp.word_tokenize(s)\n",
    "    dep=dependency(s)\n",
    "    tags=[]\n",
    "    for item in nlp.pos_tag(s):\n",
    "        tags.append(item[1])\n",
    "    lemmas=lemma(s)\n",
    "    gov=[]\n",
    "    dpg=[]\n",
    "    govw=[]\n",
    "    for w in dep:\n",
    "        gov.append(w[1])\n",
    "        dpg.append(w[0])\n",
    "        govw.append(words[w[1]-1])\n",
    "    length=len(words)+1\n",
    "    ifhead=[]\n",
    "    \n",
    "    for n in range(1,length): \n",
    "        if n in gov:\n",
    "            ifhead.append('head')\n",
    "        else:\n",
    "            ifhead.append('nonehead')\n",
    "        \n",
    "    tup=zip(words,tags,gov,lemmas,dpg,range(1,length),govw,ifhead)\n",
    "    output=[]\n",
    "    for item in tup:\n",
    "        output.append(item)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NT_unit(s): #计算T unit 结构\n",
    "    n=0\n",
    "    #print(nlp.parse(s))\n",
    "    \n",
    "    parse=nlp.parse(s).split('\\r\\n')\n",
    "    print(parse)\n",
    "    \n",
    "    for no in range(0,len(parse)):\n",
    "        #print(line)\n",
    "        if no!=len(parse) and parse[no][-2:]=='(S' and parse[no+1][-2:]!='(S':\n",
    "            n+=1\n",
    "            print(parse[no])\n",
    "        if parse[no].replace(' ','') in ('(SBAR','(TO'):\n",
    "            n=n-1\n",
    "            #print(line)\n",
    "    return n\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nverb(s): #计算从句数量\n",
    "    n=0\n",
    "    #print(nlp.parse(s))\n",
    "    parse=nlp.parse(s).split()\n",
    "    #print(parse)\n",
    "    for line in parse:\n",
    "        #print(line)\n",
    "        if line in ('(VBP','(VBZ','(VBD','(MD'):\n",
    "            n+=1\n",
    "            #print(line)\n",
    "        #if line in ('(TO'):\n",
    "        #    n=n-1\n",
    "            #print(line)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ndpclause(s): #计算关系从句数量\n",
    "    n=0\n",
    "    #print(nlp.parse(s))\n",
    "    parse=nlp.parse(s).split()\n",
    "    #print(parse)\n",
    "    for line in parse:\n",
    "        #print(line)\n",
    "        if line=='(SBAR':\n",
    "            n+=1\n",
    "            #print(line)\n",
    "        #if line in ('(TO'):\n",
    "        #    n=n-1\n",
    "            #print(line)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\r\n",
      "  (S\r\n",
      "    (NP (PRP I))\r\n",
      "    (VP (VBP like)\r\n",
      "      (NP (NN coffee)))\r\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "s='I like coffee.'\n",
    "print(nlp.parse(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fragsT(s): #定位了各类括号，找到句子中的各种结构\n",
    "    parse=nlp.parse(s) #.replace(' ','')\n",
    "    parsepos=\"\"\n",
    "    frags=[]\n",
    "    print(parse)\n",
    "    cln=[]\n",
    "    crn=[]\n",
    "    x1=0\n",
    "    #x2=0\n",
    "    #y1=0\n",
    "    y2=0\n",
    "    x0=0\n",
    "    for n1 in range(0,len(parse)):\n",
    "        if parse[n1]=='(':\n",
    "            t=0\n",
    "            t0=0\n",
    "            while n1-t0-1>=0 and parse[n1-1-t0]==' ':\n",
    "                t0+=1\n",
    "            if parse[n1-1-t0]=='\\n' :\n",
    "                t=t0\n",
    "            else:\n",
    "                t=0\n",
    "            x1+=1\n",
    "            y1=0\n",
    "            for cha in parse[n1:]:\n",
    "                if cha==\")\":\n",
    "                    y1+=1\n",
    "            cln.append(((n1,x1-1,y1),t))\n",
    "        elif parse[n1]==\")\":\n",
    "            x2=x1\n",
    "            y2=0\n",
    "            for cha in parse[n1:]:\n",
    "                if cha==\")\":\n",
    "                    y2+=1\n",
    "            crn.append((n1,x1,y2-1))\n",
    "    #print((cln,crn))\n",
    "    #print(len(cln),len(crn))\n",
    "    for cl in cln:\n",
    "        frag=\"\"\n",
    "        n=0\n",
    "        while frag==\"\" and n<=len(crn):\n",
    "                cr=crn[n]\n",
    "                if cl[0][0]<cr[0] and  cl[0][1]-cr[2]==cr[1]-cl[0][2]:\n",
    "                    #print(((cl,cr)))\n",
    "                    frag=parse[cl[0][0]:cr[0]+1]\n",
    "                    frags.append((frag,cl[1]))\n",
    "                n+=1\n",
    "    result=[frags[0]]\n",
    "    for n in range(1,len(frags)):\n",
    "        n1=n\n",
    "        while n1>=0 and frags[n1][1]==0:\n",
    "            n1=n1-1\n",
    "        result.append((frags[n][0],frags[n1][1]))\n",
    "    return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(lx,ly):\n",
    "    if len(lx)>=len(ly):\n",
    "        l1=lx\n",
    "        l2=ly\n",
    "    else:\n",
    "        l1=ly\n",
    "        l2=lx\n",
    "    for n1 in range(0,len(l1)):\n",
    "        if l1[n1]==l2[0]:\n",
    "            if l1[n1:n1+len(l2)]==l2:\n",
    "                return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frags(s): #定位了各类括号，找到句子中的各种结构 #还是不对\n",
    "    parse=nlp.parse(s).replace('\\r\\n','') #.replace(' ','')\n",
    "    parsepos=\"\"\n",
    "    frags=[]\n",
    "    print(parse)\n",
    "    cln=[]\n",
    "    crn=[]\n",
    "    x1=0\n",
    "    #x2=0\n",
    "    #y1=0\n",
    "    y2=0\n",
    "    x0=0\n",
    "    for n1 in range(0,len(parse)):\n",
    "        if parse[n1]=='(':\n",
    "            x1+=1\n",
    "            y1=0\n",
    "            for cha in parse[n1:]:\n",
    "                if cha==\")\":\n",
    "                    y1+=1\n",
    "            cln.append((n1,x1-1,y1))\n",
    "        elif parse[n1]==\")\":\n",
    "            x2=x1\n",
    "            y2=0\n",
    "            for cha in parse[n1:]:\n",
    "                if cha==\")\":\n",
    "                    y2+=1\n",
    "            crn.append((n1,x1,y2-1))\n",
    "    #print((cln,crn))\n",
    "    #print(len(cln),len(crn))\n",
    "    for cl in cln:\n",
    "        frag=\"\"\n",
    "        n=0\n",
    "        while frag==\"\" and n<=len(crn):\n",
    "                cr=crn[n]\n",
    "                if cl[0]<cr[0] and  cl[1]-cr[2]==cr[1]-cl[2]:\n",
    "                    #print(((cl,cr)))\n",
    "                    frag=parse[cl[0]:cr[0]+1]\n",
    "                    frags.append(frag)\n",
    "                n+=1\n",
    "    return frags\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clause_component(s):\n",
    "    \n",
    "    target=frags(s)\n",
    "    #print(target)\n",
    "    result=[]\n",
    "    for c in target:\n",
    "        c=c.replace(\"(\",\" \").replace(\")\",' ')\n",
    "        c=c.split(' ')\n",
    "        templ=[]\n",
    "        for com in c:\n",
    "            if com!='':\n",
    "                templ.append(com)\n",
    "        result.append(templ)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clause_componentT(s):\n",
    "    \n",
    "    target=fragsT(s)\n",
    "    print(target)\n",
    "    result=[]\n",
    "    for c in target:\n",
    "        com=c[0].replace(\"(\",\" \").replace(\")\",' ').replace('\\r\\n',\"\")\n",
    "        com=com.split(' ')\n",
    "        templ=[]\n",
    "        for p in com:\n",
    "            if p!='':\n",
    "                templ.append((p,c[1]))\n",
    "        result.append(templ)\n",
    "    result1=[]\n",
    "    templ1=[]\n",
    "    #print(result) \n",
    "    for t in result[0]: #[('NN', 8), ('beauty', 8)]\n",
    "        #for t in l: #('ROOT', 0)\n",
    "            print(t[0]) \n",
    "            templ1.append(t[0])\n",
    "    result1.append((templ1,(result[0][0][1],0)))\n",
    "    for n in range(1,len(result)):\n",
    "        templ=[]\n",
    "        for c in result[n]: #result[n]是个由成分和从属组成的tuple的列表\n",
    "            templ.append(c[0])\n",
    "        n1=n-1\n",
    "        #print('【check】',result[n][0][1],\",\",result[n1][0][1])\n",
    "        while n1>=0 and result[n1][0][1]>=result[n][0][1]:\n",
    "            #print('【check】',result[n][0][1],\",\",result[n1][0][1])\n",
    "            n1=n1-1\n",
    "        result1.append((templ,(result[n1][0][0],n1)))\n",
    "    return result1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_np(s):\n",
    "    target=frags(s)\n",
    "    \n",
    "    result=[]\n",
    "    for c in target:\n",
    "        print(c)\n",
    "        c=c.replace(\"(\",\" \").replace(\")\",' ')\n",
    "        c=c.split(' ')\n",
    "        if c!='':\n",
    "            result.append(c)\n",
    "        #print(c)\n",
    "    rresult=[]\n",
    "    for c in result:\n",
    "        if c[1]=='NP':\n",
    "            rresult.append(c)\n",
    "    rrresult=[]\n",
    "    for c1 in rresult:\n",
    "        for c2 in rresult:\n",
    "            if not match(c1,c2):\n",
    "                rrresult.append(c1)\n",
    "    return rresult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_np(s):\n",
    "    target=frags(s)\n",
    "    \n",
    "    result=[]\n",
    "    for c in target:\n",
    "        if c[:3]=='(NP':\n",
    "            result.append(c)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cnp(s): #find complex np\n",
    "    target=find_np(s)\n",
    "    result=[]\n",
    "    for c1 in target:\n",
    "        n=0\n",
    "        for c2 in target:\n",
    "            if c1 in c2:\n",
    "                n+=1\n",
    "        if n<=1:\n",
    "            for c2 in (\"(PP\",'(VBN','JJ','SBAR','(S(','(VBG('):\n",
    "                if c2 in c1 and c1 not in result:\n",
    "                    result.append(c1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ncnp(s):\n",
    "    target=find_cnp(s)\n",
    "    return len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdsyncomT2(s): #开始修改\n",
    "    CN=[] #子句数量\n",
    "    SBARS=[] #依存从句数量\n",
    "    RS=[] #sum of root 句子数量\n",
    "    VPS=[]\n",
    "    TN=[] #T-unit 结构 \n",
    "    CTN=[] #复杂T-unit 结构 \n",
    "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
    "    \n",
    "    for n in range(0,len(target)):\n",
    "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
    "        if target[n][0][0]=='ROOT':\n",
    "            RS.append(target[n])\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
    "                    TN.append(target[n1])  #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
    "                    if 'VP' not in target[n][0]:\n",
    "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
    "                    \n",
    "                    \n",
    "            \n",
    "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
    "        if target[n][0][0] in ('S','SINV','SQ'):\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][1][1]==n: \n",
    "                    if target[n1][0][0]=='VP':\n",
    "                        #CN.append(target[n])\n",
    "                        if target[n1][0][1] in ('VBD','VBP','VBZ','MD'):\n",
    "                            CN.append(target[n])\n",
    "        \n",
    "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
    "        if target[n][0][0] =='SBAR':\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][1][1]==n: \n",
    "                    if target[n1][0][0] in ('S','SINV','SQ'):\n",
    "                        for n2 in range(n1+1,len(target)):\n",
    "                            if target[n2][1][1]==n1:\n",
    "                                if target[n2][0][0]=='VP':\n",
    "                                    #SBARS.append(target[n])\n",
    "                                    if target[n2][0][1] in('VBD','VBP','VBZ','MD'):\n",
    "                                        SBARS.append(target[n])\n",
    "        \n",
    "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
    "        if target[n][0][0]=='ROOT':\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][1][1]==n: \n",
    "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
    "                        TN.append(target[n1])\n",
    "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
    "                        for n2 in range(1,len(target[n1][0])):\n",
    "                            if target[n1][0][n2]=='SBAR':\n",
    "                                tempn=1\n",
    "                                n3=n2+tempn\n",
    "                                while n3<=len(target[n1][0]) and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
    "                                    n3+=1\n",
    "                                if n3<=len(target[n1][0]):\n",
    "                                    CTN.append(target[n1])\n",
    "                                    \n",
    "                            \n",
    "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
    "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
    "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
    "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
    "                    tempn=0\n",
    "                    for n2 in range(0,len(target)): #target[n2] 查看 是否从属于 从句结构\n",
    "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
    "                            if len(target[n2][0])>len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
    "                        #如果前面计数是0 且 存在并列结构\n",
    "                                tempn+=1\n",
    "                    if tempn==0:\n",
    "                        TN.append(target[n])\n",
    "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
    "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
    "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
    "                                tempn=1\n",
    "                                n4=n3+tempn\n",
    "                                while n4<=len(target[n][0]) and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
    "                                    n4+=1\n",
    "                                if n4<=len(target[n][0]):\n",
    "                                    CTN.append(target[n])\n",
    "                        \n",
    "                                        \n",
    "                                        \n",
    "\n",
    "\n",
    "############################################################至此完成        \n",
    "\n",
    "\n",
    "    \n",
    "    CNN=Ncnp(s)\n",
    "    VN=Nverb(s)\n",
    "    print('\\n\\n')\n",
    "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
    "    print('1. SENTENCE NUM:','\\t',RS)\n",
    "    print('2. CLAUSE NUM:','\\t',CN)\n",
    "    print('3. T_UNIT NUM:','\\t',TN)\n",
    "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
    "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
    "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
    "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
    "    print('\\n\\n')\n",
    "    print(s)\n",
    "    print(nlp.parse(s))\n",
    "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
    "    print('\\n\\n')\n",
    "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
    "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
    "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
    "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
    "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
    "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
    "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
    "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdclausecom(s): \n",
    "    SS=0\n",
    "    SBARS=0\n",
    "    RS=0\n",
    "    VPS=0\n",
    "    CTN=0\n",
    "    target=find_clause_component(s)\n",
    "    for line in target:\n",
    "        if line[1]=='ROOT':\n",
    "            RS+=1\n",
    "        if line[1]=='S' and len(line)<=2 or line[1]=='S'and line[2]!='S':\n",
    "            SS+=1\n",
    "            if 'SBAR' in line:\n",
    "                CTN+=1\n",
    "        if line[1]=='SBAR':\n",
    "            SBARS+=1\n",
    "        if line[1]=='VP':\n",
    "            VPS+=1\n",
    "    TN=SS-SBARS\n",
    "    CN=SS\n",
    "    CNN=Ncnp(s)\n",
    "    VN=Nverb(s)\n",
    "    print('SENTENCE NUM:',RS)\n",
    "    print('CLAUSE NUM:',CN)\n",
    "    print('T_UNIT NUM:',TN)\n",
    "    print('DEPENDENT CLAUSE NUM:',SBARS)\n",
    "    print('COMPLEX T_UNIT NUM:',CTN)\n",
    "    print('COMPLEX NOUN PHRASE NUM:',CNN)\n",
    "    print('VERB PHRASE NUM:',VN)\n",
    "    return(RS,CN,TN,SBARS,CTN,CNN,VN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdsyncomT1(s): #开始修改\n",
    "    CN=0 #子句数量\n",
    "    SBARS=0 #依存从句数量\n",
    "    RS=0 #sum of root 句子数量\n",
    "    VPS=0\n",
    "    TN=0 #T-unit 结构 \n",
    "    CTN=0 #复杂T-unit 结构 \n",
    "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
    "    \n",
    "    for n in range(0,len(target)):\n",
    "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
    "        if target[n][0][0]=='ROOT':\n",
    "            RS+=1\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
    "                    TN+=1  #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
    "                    if 'VP' not in target[n][0]:\n",
    "                        CN+=1  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
    "                    \n",
    "                    \n",
    "            \n",
    "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
    "        if target[n][0][0] in ('S','SINV','SQ'):\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][1][1]==n: \n",
    "                    if target[n1][0][0] in ('VBD','VBP','VBZ'):\n",
    "                        CN+=1\n",
    "                    if target[n1][0][0]=='MD' and target[n1][0][1]=='VP':\n",
    "                        CN+=1\n",
    "        \n",
    "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
    "        if target[n][0][0] =='SBAR':\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][1][1]==n: \n",
    "                    if target[n1][0][0] in ('S','SINV','SQ'):\n",
    "                        for n2 in range(n1+1,len(target)):\n",
    "                            if target[n2][1][1]==n1:\n",
    "                                if target[n2][0][0] in ('VBD','VBP','VBZ'):\n",
    "                                    SBARS+=1\n",
    "                                if target[n2][0][:2]==('VP','MD'):\n",
    "                                    SBARS+=1\n",
    "        \n",
    "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
    "        if target[n][0][0]=='ROOT':\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][1][1]==n: \n",
    "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
    "                        TN+=1\n",
    "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
    "                        for n2 in range(1,len(target[n1][0])):\n",
    "                            if target[n1][0][n2]=='SBAR':\n",
    "                                tempn=1\n",
    "                                n3=n2+tempn\n",
    "                                while n3<=len(target[n1][0]) and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
    "                                    n3+=1\n",
    "                                if n3<=len(target[n1][0]):\n",
    "                                    CTN+=1\n",
    "                                    \n",
    "                            \n",
    "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
    "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
    "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
    "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
    "                    tempn=0\n",
    "                    for n2 in range(0,len(target)): #target[n2] 查看 是否从属于 从句结构\n",
    "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
    "                            if len(target[n2][0])>len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
    "                        #如果前面计数是0 且 存在并列结构\n",
    "                                tempn+=1\n",
    "                    if tempn==0:\n",
    "                        TN+=1\n",
    "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
    "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
    "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
    "                                tempn=1\n",
    "                                n4=n3+tempn\n",
    "                                while n4<=len(target[n][0]) and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
    "                                    n4+=1\n",
    "                                if n4<=len(target[n][0]):\n",
    "                                    CTN+=1\n",
    "                        \n",
    "                                        \n",
    "                                        \n",
    "\n",
    "\n",
    "############################################################至此完成        \n",
    "\n",
    "\n",
    "    \n",
    "    CNN=Ncnp(s)\n",
    "    VN=Nverb(s)\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print(s)\n",
    "    print(nlp.parse(s))\n",
    "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
    "    print('\\n\\n')\n",
    "    print('1. SENTENCE NUM:','\\t',RS)\n",
    "    print('2. CLAUSE NUM:','\\t',CN)\n",
    "    print('3. T_UNIT NUM:','\\t',TN)\n",
    "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
    "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
    "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
    "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
    "    return(RS,CN,TN,SBARS,CTN,CNN,VN)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdsyncomT3(s): #暂时完成！！\n",
    "    CN=[] #子句数量\n",
    "    SBARS=[] #依存从句数量\n",
    "    RS=[] #sum of root 句子数量\n",
    "    VPS=[]\n",
    "    TN=[] #T-unit 结构 \n",
    "    CTN=[] #复杂T-unit 结构 \n",
    "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
    "    \n",
    "    for n in range(0,len(target)):\n",
    "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
    "        if target[n][0][0]=='ROOT':\n",
    "            RS.append(target[n])\n",
    "            for n1 in range(n+1,len(target)-1):\n",
    "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
    "                    if 'VP' in target[n][0]:\n",
    "                        TN.append(target[n1])  #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
    "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
    "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
    "                                n3=n2+1\n",
    "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
    "                                    n3+=1\n",
    "                                if n3<len(target[n1][0]):\n",
    "                                    CTN.append(target[n1])\n",
    "                    else:\n",
    "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
    "                    \n",
    "                    \n",
    "            \n",
    "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
    "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][0][0]=='VP': \n",
    "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
    "                        #CN.append(target[n])\n",
    "                        n2=1\n",
    "                        while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
    "                            print('【check】',n2,target[n1],target[n1][0][n2])\n",
    "                            n2+=1\n",
    "                        if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
    "                            if target[n] not in CN:\n",
    "                                CN.append(target[n])\n",
    "\n",
    "                    else:                  #VP间接从属于 S不能计算入内\n",
    "                        for n3 in range(1,n1-1): \n",
    "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
    "                                n2=1\n",
    "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
    "                                    n2+=1\n",
    "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
    "                                    if target[n] not in CN:\n",
    "                                        CN=CN\n",
    "                                        #CN.append(target[n])\n",
    "                                \n",
    "        \n",
    "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
    "        if target[n][0][0] =='SBAR':\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][1][1]==n: \n",
    "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
    "                        for n2 in range(n1+1,len(target)):\n",
    "                            if target[n2][0][0]=='VP': #VP直接从属于 S \n",
    "                                if target[n2][1][1]==n1:\n",
    "                                    n3=1\n",
    "                                    while n3<=len(target[n2][0])-1 and target[n2][0][n3] not in ('VBD','VBP','VBZ','MD'):\n",
    "                                        n3+=1\n",
    "                                    if n3<len(target[n2][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
    "                                        if target[n] not in SBARS:\n",
    "                                            SBARS.append(target[n])\n",
    "                                else:                  #VP间接从属于 S\n",
    "                                    for n4 in range(1,n2-1): \n",
    "                                        if target[n2][1][1]==n4 and target[n4][1][1]==n1:\n",
    "                                            n5=1\n",
    "                                            while n5<=len(target[n2][0])-1 and target[n2][0][n5] not in ('VBD','VBP','VBZ','MD'):\n",
    "                                                n5+=1\n",
    "                                            if n5<len(target[n2][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
    "                                                if target[n] not in SBARS:\n",
    "                                                    SBARS.append(target[n])\n",
    "                                        \n",
    "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
    "        if target[n][0][0]=='ROOT':\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][1][1]==n: \n",
    "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
    "                        if target[n1] not in TN:\n",
    "                            TN.append(target[n1])\n",
    "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
    "                        for n2 in range(1,len(target[n1][0])):\n",
    "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
    "                                n3=n2+1\n",
    "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
    "                                    n3+=1\n",
    "                                if n3<len(target[n1][0]):\n",
    "                                    if target[n1] not in CTN:\n",
    "                                        CTN.append(target[n1])\n",
    "                                    \n",
    "                            \n",
    "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
    "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
    "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
    "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
    "                    tempn=0\n",
    "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
    "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
    "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
    "                        #如果前面计数是0 且 存在并列结构\n",
    "                                tempn+=1\n",
    "                    if tempn==0:\n",
    "                        TN.append(target[n])\n",
    "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
    "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
    "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
    "                                n4=n3+1\n",
    "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
    "                                    n4+=1\n",
    "                                if n4<len(target[n][0]):\n",
    "                                    CTN.append(target[n])\n",
    "                        \n",
    "                                        \n",
    "                                        \n",
    "\n",
    "\n",
    "############################################################至此完成        \n",
    "\n",
    "\n",
    "    \n",
    "    CNN=Ncnp(s)\n",
    "    VN=Nverb(s)\n",
    "    print('\\n\\n')\n",
    "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
    "    print('1. SENTENCE NUM:','\\t',RS)\n",
    "    print('2. CLAUSE NUM:','\\t',CN)\n",
    "    print('3. T_UNIT NUM:','\\t',TN)\n",
    "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
    "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
    "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
    "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
    "    print('\\n\\n')\n",
    "    print(s)\n",
    "    print(nlp.parse(s))\n",
    "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
    "    print('\\n\\n')\n",
    "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
    "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
    "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
    "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
    "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
    "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
    "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
    "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdsyncomT4(s): #完成！！\n",
    "    CN=[] #子句数量\n",
    "    SBARS=[] #依存从句数量\n",
    "    RS=[] #sum of root 句子数量\n",
    "    VPS=[]\n",
    "    TN=[] #T-unit 结构 \n",
    "    CTN=[] #复杂T-unit 结构 \n",
    "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
    "    \n",
    "    for n in range(0,len(target)):\n",
    "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
    "        if target[n][0][0]=='ROOT':\n",
    "            RS.append(target[n])\n",
    "            for n1 in range(n+1,len(target)-1):\n",
    "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
    "                    if 'VP' in target[n][0]:\n",
    "                        TN.append(target[n1])  \n",
    "                        #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
    "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
    "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
    "                                n3=n2+1\n",
    "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
    "                                    n3+=1\n",
    "                                if n3<len(target[n1][0]):\n",
    "                                    CTN.append(target[n1])\n",
    "                    else:\n",
    "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
    "                    \n",
    "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
    "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][0][0]=='VP' and target[n1][0][1] in ('VBD','VBP','VBZ','MD'):\n",
    "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
    "                        if target[n] not in CN:\n",
    "                            CN.append(target[n])  #VP下面直接存在 VBD VBP VBZ 和 MD \n",
    "\n",
    "                    else:                  #VP间接从属于 S不能计算入内\n",
    "                        for n3 in range(1,n1-1): \n",
    "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
    "                                n2=1\n",
    "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
    "                                    n2+=1\n",
    "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
    "                                    if target[n] not in CN:\n",
    "                                        CN=CN\n",
    "                                        #CN.append(target[n])\n",
    "                                \n",
    "        \n",
    "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
    "        if target[n][0][0] =='SBAR':\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][1][1]==n: \n",
    "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
    "                        for n2 in range(n1+1,len(target)):\n",
    "                            if target[n2][0][0]=='VP' and target[n2][0][1] in ('VBD','VBP','VBZ','MD'): #VP直接从属于 S \n",
    "                                    if target[n] not in SBARS:\n",
    "                                        SBARS.append(target[n])\n",
    "                                #不考虑VP间接从属于 S\n",
    "\n",
    "                                        \n",
    "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
    "        if target[n][0][0]=='ROOT':\n",
    "            for n1 in range(n+1,len(target)):\n",
    "                if target[n1][1][1]==n: \n",
    "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
    "                        if target[n1] not in TN:\n",
    "                            TN.append(target[n1])\n",
    "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
    "                        for n2 in range(1,len(target[n1][0])):\n",
    "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
    "                                n3=n2+1\n",
    "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
    "                                    n3+=1\n",
    "                                if n3<len(target[n1][0]):\n",
    "                                    if target[n1] not in CTN:\n",
    "                                        CTN.append(target[n1])\n",
    "                                    \n",
    "                            \n",
    "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
    "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
    "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
    "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
    "                    tempn=0\n",
    "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
    "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
    "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
    "                        #如果前面计数是0 且 存在并列结构\n",
    "                                tempn+=1\n",
    "                    if tempn==0:\n",
    "                        TN.append(target[n])\n",
    "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
    "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
    "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
    "                                n4=n3+1\n",
    "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
    "                                    n4+=1\n",
    "                                if n4<len(target[n][0]):\n",
    "                                    CTN.append(target[n])\n",
    "                        \n",
    "                                        \n",
    "                                        \n",
    "\n",
    "\n",
    "############################################################至此完成        \n",
    "\n",
    "\n",
    "    \n",
    "    CNN=Ncnp(s)\n",
    "    VN=Nverb(s)\n",
    "    print('\\n\\n')\n",
    "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
    "    print('1. SENTENCE NUM:','\\t',RS)\n",
    "    print('2. CLAUSE NUM:','\\t',CN)\n",
    "    print('3. T_UNIT NUM:','\\t',TN)\n",
    "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
    "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
    "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
    "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
    "    print('\\n\\n')\n",
    "    print(s)\n",
    "    print(nlp.parse(s))\n",
    "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
    "    print('\\n\\n')\n",
    "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
    "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
    "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
    "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
    "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
    "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
    "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
    "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readsent(file):\n",
    "    list_sents=[]\n",
    "    \n",
    "    text=open(file,encoding='utf-8',errors='ignore')\n",
    "        \n",
    "    for line in text:\n",
    "        line=line.strip()\n",
    "        sent=nltk.sent_tokenize(line)\n",
    "        \n",
    "        list_sents+=sent\n",
    "    return list_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SenComAnalyser3(file):\n",
    "    target=readsent(file)\n",
    "    data=[]\n",
    "    sumWN=[]\n",
    "    result=''\n",
    "    datatitle=['W','S','C','T','DC','CT','CN','VP']\n",
    "    headline='SENT\\COMP'\n",
    "    for t in datatitle:\n",
    "        headline+='\\t'+t\n",
    "    result+=headline+'\\n'\n",
    "    print(headline)\n",
    "    n=1\n",
    "    for line in target:\n",
    "        WN=len(nltk.word_tokenize(line))\n",
    "        l=''\n",
    "        l+='sentence'+str(n)+'\\t'\n",
    "        l+=str(WN)+'\\t'\n",
    "        for d in fdclausecom(line):\n",
    "            l+=str(d)+'\\t'\n",
    "        result+=l+'\\n'\n",
    "        n+=1\n",
    "        #print('sentence'+str(n),'\\t',l)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SenComAnalyser4(file):\n",
    "    target=readsent(file)\n",
    "    data=[]\n",
    "    sumWN=[]\n",
    "    result=''\n",
    "    datatitle=['W','S','C','T','DC','CT','CN','VP']\n",
    "    headline='SENT\\COMP'\n",
    "    for t in datatitle:\n",
    "        headline+='\\t'+t\n",
    "    result+=headline+'\\n'\n",
    "    print(headline)\n",
    "    n=1\n",
    "    for line in target:\n",
    "        WN=len(nltk.word_tokenize(line))\n",
    "        l=''\n",
    "        l+='sentence'+str(n)+'\\t'\n",
    "        l+=str(WN)+'\\t'\n",
    "        for d in fdsyncomT3(line):\n",
    "            l+=str(d)+'\\t'\n",
    "        result+=l+'\\n'\n",
    "        n+=1\n",
    "        #print('sentence'+str(n),'\\t',l)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SenComAnalyser3(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SenComAnalyser1(file):\n",
    "    target=readsent(file)\n",
    "    data=[]\n",
    "    sumWN=[]\n",
    "    for line in target:\n",
    "        WN=len(nltk.word_tokenize(line))\n",
    "        sumWN.append(WN)\n",
    "        data.append(fdclausecom(line))\n",
    "    \n",
    "    S=''\n",
    "    T=''\n",
    "    C=\"\"\n",
    "    CT=\"\"\n",
    "    DC=\"\"\n",
    "    CN=\"\"\n",
    "    VP=\"\"\n",
    "    datas=[S,C,T,DC,CT,CN,VP]\n",
    "    datatitle=['W','S','C','T','DC','CT','CN','VP']\n",
    "    n=0\n",
    "    sumresult=[]\n",
    "    W=''\n",
    "    for c in sumWN:\n",
    "         W+=str(c)+'\\t'\n",
    "    sumresult.append(W)\n",
    "    for l in datas:\n",
    "        for c in data:\n",
    "            #print(c)\n",
    "            l+=str(c[n])+'\\t'\n",
    "        #print(l)\n",
    "        sumresult.append(l)\n",
    "        n+=1\n",
    "    headline='components'\n",
    "    for n in range(1,len(target)+1):\n",
    "        headline+='\\t'+'sentence'+str(n)\n",
    "    print(headline)\n",
    "    for n in range(0,len(sumresult)):\n",
    "        print(datatitle[n],'\\t',sumresult[n])\n",
    "       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SenComAnalyser2(file):\n",
    "    target=readsent(file)\n",
    "    data=[]\n",
    "    sumWN=[]\n",
    "    result=''\n",
    "    datatitle=[\"W\",\"W/T\",\"W/C\",\"C\",\"C/T\",\"CT/T\",\"DC/C\",\"DC/T\",\"T\",'CN',\"CN/C\",\"CN/T\",\"VP/C\"]\n",
    "    headline='\\t长度特征 \\t \\t \\t子句数量 \\t从属情况 \\t \\t \\t \\t并列情况 \\t特殊结构占比\\t \\t \\t'\n",
    "    result+=headline+'\\n'\n",
    "    headline='SENT\\COMP'\n",
    "    for t in datatitle:\n",
    "        headline+='\\t'+t\n",
    "    result+=headline+'\\n'\n",
    "    print(headline)\n",
    "    n=1\n",
    "    \n",
    "        \n",
    "    for line in target:\n",
    "        w=len(nltk.word_tokenize(line))\n",
    "        l=''\n",
    "        l+='sentence'+str(n)+'\\t'\n",
    "        l+=str(w)+'\\t'   #W\n",
    "        data=fdclausecom(line)\n",
    "        if data[2] !=0 :       #W/T\n",
    "            l+=str(round(w/data[2],2))  +'\\t' \n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[1] !=0 :  #W/C\n",
    "            l+=str(round(w/data[1],2))  +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        l+=str(data[1])+'\\t'   #C\n",
    "\n",
    "        if data[2] !=0 :\n",
    "            l+= str(round(data[1]/data[2],2))    +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[2] !=0 :\n",
    "            l+= str(round(data[4]/data[2],2))    +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[1] !=0 :\n",
    "            l+= str(round(data[3]/data[1],2))    +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[2] !=0 :\n",
    "            l+= str(round(data[3]/data[2],2))   +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        l+=str(data[2])+'\\t'\n",
    "        l+=str(data[5])+'\\t'\n",
    "        if data[1] !=0 :\n",
    "            l+= str(round(data[5]/data[1],2)) +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[2] !=0 :\n",
    "            l+= str(round(data[5]/data[2],2)) +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[1] !=0 :\n",
    "            l+= str(round(data[6]/data[1],2)) +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        result+=l+'\\n'\n",
    "        n+=1\n",
    "        #print('sentence'+str(n),'\\t',l)\n",
    "    print(result)\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fdsyncomT4(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SenComAnalyser5(file):\n",
    "    target=readsent(file)\n",
    "    data=[]\n",
    "    sumWN=[]\n",
    "    result=''\n",
    "    datatitle=[\"W\",\"W/T\",\"W/C\",\"C\",\"C/T\",\"CT/T\",\"DC/C\",\"DC/T\",\"T\",'CN',\"CN/C\",\"CN/T\",\"VP/C\"]\n",
    "    headline='\\t长度特征 \\t \\t \\t子句数量 \\t从属情况 \\t \\t \\t \\t并列情况 \\t特殊结构占比\\t \\t \\t'\n",
    "    result+=headline+'\\n'\n",
    "    headline='SENT\\COMP'\n",
    "    for t in datatitle:\n",
    "        headline+='\\t'+t\n",
    "    result+=headline+'\\n'\n",
    "    print(headline)\n",
    "    n=1\n",
    "    W=0\n",
    "    T=0\n",
    "    C=0\n",
    "    DC=0\n",
    "    CT=0\n",
    "    CN=0\n",
    "    VP=0\n",
    "    for line in target:\n",
    "        w=len(nltk.word_tokenize(line))\n",
    "        l=''\n",
    "        l+='sentence'+str(n)+'\\t'\n",
    "        l+=str(w)+'\\t'   #W\n",
    "        W+=w\n",
    "        data=fdsyncomT4(line)\n",
    "        C+=data[1]\n",
    "        T+=data[2]\n",
    "        DC+=data[3]\n",
    "        CT+=data[4]\n",
    "        CN+=data[5]\n",
    "        VP+=data[6]\n",
    "        if data[2] !=0 :       #W/T\n",
    "            l+=str(round(w/data[2],2))  +'\\t' \n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[1] !=0 :  #W/C\n",
    "            l+=str(round(w/data[1],2))  +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        l+=str(data[1])+'\\t'   #C\n",
    "        \n",
    "        if data[2] !=0 :\n",
    "            l+= str(round(data[1]/data[2],2))    +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[2] !=0 :\n",
    "            l+= str(round(data[4]/data[2],2))    +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[1] !=0 :\n",
    "            l+= str(round(data[3]/data[1],2))    +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[2] !=0 :\n",
    "            l+= str(round(data[3]/data[2],2))   +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        l+=str(data[2])+'\\t'\n",
    "        l+=str(data[5])+'\\t'\n",
    "        if data[1] !=0 :\n",
    "            l+= str(round(data[5]/data[1],2)) +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[2] !=0 :\n",
    "            l+= str(round(data[5]/data[2],2)) +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        if data[1] !=0 :\n",
    "            l+= str(round(data[6]/data[1],2)) +'\\t'\n",
    "        else:\n",
    "            l+='0'+'\\t'\n",
    "        result+=l+'\\n'\n",
    "        n+=1\n",
    "        #print('sentence'+str(n),'\\t',l)\n",
    "    print(result)\n",
    "    dataresult=(W,round(W/T,2),round(W/C,2),round(C/T,2),round(CT/T,2),round(DC/C,2),round(DC/T,2),T,CN,round(CN/C,2),round(CN/T,2),round(VP/C,2))\n",
    "    return((W,T,C,DC,CT,CN,VP),(dataresult))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fdsyncomT3(readsent(file)[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
    "#SenComAnalyser5(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.tree import Tree\n",
    "#s=readsent(file)[6]\n",
    "#s=text\n",
    "#deparse= nlp.parse(s)\n",
    "#tree=Tree.fromstring(deparse)\n",
    "#tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stanz_text(txt): #将段落转换为数据表\n",
    "    output=[]\n",
    "    txt=nltk.sent_tokenize(txt)\n",
    "    for sent in txt:\n",
    "        output.append(stanz(sent))\n",
    "    prtline=''\n",
    "    text=''\n",
    "    for line in output:\n",
    "        #prtline+=\"\\n\"\n",
    "        for item in line:\n",
    "            if item[0] in string.punctuation:\n",
    "                text+=item[0]\n",
    "            else:\n",
    "                text+=' '+item[0]\n",
    "            for inf in item:\n",
    "                prtline+=str(inf)+'\\t'\n",
    "            prtline+='\\n'\n",
    "        prtline+=50*'-'+'\\n'\n",
    "    print(prtline)\n",
    "    print(text[1:])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = 'Unable to resist the impulse of curiosity, she raised her eyes to his.' #stanford NLP 示范\n",
    "#doc=xnlp(s)\n",
    "#print (lemma(s))\n",
    "#print(gonver(s))\n",
    "#print(*[f\"index: {word.index.rjust(2)}\\tword: {word.text.ljust(11)}\\tgovernor index: {word.governor}\\tgovernor: {(doc.sentences[0].words[word.governor-1].text if word.governor > 0 else 'root').ljust(11)}\\tdeprel: {word.dependency_relation}\" for word in doc.sentences[0].words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1='In a study of 33 years of trends in Body Mass Index  across 200 countries, the scientists found that people worldwide are getting heavier and that most of the rise is due to gains in BMI in rural areas.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2='In 1985, urban men and women in more than three quarters of the countries studied had higher BMIs than men and women in rural areas.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tree import Tree\n",
    "deparse= nlp.parse(s)\n",
    "tree=Tree.fromstring(deparse)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(ROOT', '(S', '(S', '(VP', '(VBG', 'Going)', '(PP', '(PP', '(IN', 'from)', '(NP', '(JJ', 'junior)', '(JJ', 'high)', '(NN', 'school)))', '(PP', '(TO', 'to)', '(NP', '(JJ', 'senior)', '(JJ', 'high)', '(NN', 'school))))))', '(VP', '(VBZ', 'is)', '(NP', '(DT', 'a)', '(RB', 'really)', '(JJ', 'big)', '(NN', 'challenge)))', '(.', '.)))']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
    "#s=text\n",
    "outcome=\"\"\n",
    "deparse= nlp.parse(s)\n",
    "data=deparse.split()\n",
    "print(data)\n",
    "num=0\n",
    "for n in range(0,len(deparse)):\n",
    "    if n<=len(deparse)-2 and deparse[n]+deparse[n+1]==\"NP\" or n-1>=0 and deparse[n-1]+deparse[n]==\"NP\" or\\\n",
    "        n-3>=0 and deparse[n-3]+deparse[n-2]+deparse[n-1]+deparse[n] in ('ADJP',\"ADVP\") or\\\n",
    "        n-2>=0 and n<=len(deparse)-2 and deparse[n-2]+deparse[n-1]+deparse[n]+deparse[n+1] in ('ADJP',\"ADVP\") or\\\n",
    "        n-1>=0 and n<=len(deparse)-3 and deparse[n-1]+deparse[n]+deparse[n+1]+deparse[n+2] in ('ADJP',\"ADVP\") or\\\n",
    "        n<=len(deparse)-4 and deparse[n]+deparse[n+1]+deparse[n+2]+deparse[n+3] in ('ADJP',\"ADVP\"):\n",
    "        outcome+=\" \"\n",
    "    else:\n",
    "        outcome+=deparse[n]\n",
    "outcome2=''\n",
    "no=1\n",
    "for n in range(0,len(outcome)):\n",
    "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
    "        outcome2+=str(no)+'____'\n",
    "        no+=1\n",
    "    else:\n",
    "        outcome2+=outcome[n]\n",
    "tree=Tree.fromstring(outcome2)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(ROOT#@', '(S#@', '(S#@', '(VP', '(VBG', 'Going)#@', '(PP#@', '(PP', '(IN', 'from)#@', '(NP', '(JJ', 'junior)', '(JJ', 'high)', '(NN', 'school)))#@', '(PP', '(TO', 'to)#@', '(NP', '(JJ', 'senior)', '(JJ', 'high)', '(NN', 'school))))))#@', '(VP', '(VBZ', 'is)#@', '(NP', '(DT', 'a)', '(RB', 'really)', '(JJ', 'big)', '(NN', 'challenge)))#@', '(.', '.)))']\n",
      "(Sentence#@(((VP (Going)#@ (PP#@ (PP (from)#@ ( (junior) (high) (school)))#@ (PP (to)#@ ( (senior) (high) (school))))))#@ (VP (is)#@ ( (a) (really) (big) (challenge)))#@ (.))) \n",
      "(Sentence\r\n",
      "(((VP (Going)\r\n",
      " (PP\r\n",
      " (PP (from)\r\n",
      " ( (junior) (high) (school)))\r\n",
      " (PP (to)\r\n",
      " ( (senior) (high) (school))))))\r\n",
      " (VP (is)\r\n",
      " ( (a) (really) (big) (challenge)))\r\n",
      " (.))) \n"
     ]
    }
   ],
   "source": [
    "from nltk.tree import Tree #将句子改为短语填图练习 此处改编针对性填空练习\n",
    "s='Going from junior high school to senior high school is a really big challenge.'\n",
    "outcome=\"\"\n",
    "deparse= nlp.parse(s)\n",
    "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
    "data=deparse.split()\n",
    "print(data)\n",
    "num=0\n",
    "for n in range(0,len(data)):\n",
    "    if data[n]=='(NP' and data[n+1]!='(TO': # \n",
    "        outcome+=\"( \"\n",
    "    elif data[n] in(\"(ADJP\",\"(ADVP\",'(NP','(VP','(PP',\"(ADJP#@\",\"(ADVP#@\",'(NP#@','(VP#@','(PP#@'):\n",
    "        outcome+=data[n]+' '\n",
    "    elif data[n]=='(ROOT#@':\n",
    "        outcome+='(Sentence#@'\n",
    "    elif \")\" not in data[n]:\n",
    "        outcome+=\"(\"\n",
    "    else:\n",
    "        outcome+=data[n]+' '\n",
    "        \n",
    "print(outcome)\n",
    "\n",
    "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
    "print(outcome)\n",
    "outcome2=''\n",
    "no=1\n",
    "for n in range(0,len(outcome)):\n",
    "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
    "        outcome2+=str(no)+'____'\n",
    "        no+=1\n",
    "    else:\n",
    "        outcome2+=outcome[n]\n",
    "tree=Tree.fromstring(outcome2)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(ROOT#@', '(S#@', '(S#@', '(VP', '(VBG', 'Going)#@', '(PP#@', '(PP', '(IN', 'from)#@', '(NP', '(JJ', 'junior)', '(JJ', 'high)', '(NN', 'school)))#@', '(PP', '(TO', 'to)#@', '(NP', '(JJ', 'senior)', '(JJ', 'high)', '(NN', 'school))))))#@', '(VP', '(VBZ', 'is)#@', '(NP', '(DT', 'a)', '(RB', 'really)', '(JJ', 'big)', '(NN', 'challenge)))#@', '(.', '.)))']\n",
      "(Sentence#@(((VP (Going)#@ (PP#@ (PP (from)#@ (NP (junior) (high) (school)))#@ (PP (to)#@ (NP (senior) (high) (school))))))#@ (VP (is)#@ (NP (a) (really) (big) (challenge)))#@ (.))) \n",
      "(Sentence\r\n",
      "(((VP (Going)\r\n",
      " (PP\r\n",
      " (PP (from)\r\n",
      " (NP (junior) (high) (school)))\r\n",
      " (PP (to)\r\n",
      " (NP (senior) (high) (school))))))\r\n",
      " (VP (is)\r\n",
      " (NP (a) (really) (big) (challenge)))\r\n",
      " (.))) \n",
      "                                                 Sentence                                        \n",
      "                                                    |                                             \n",
      "                                                                                                 \n",
      "              ______________________________________|__________________________________________   \n",
      "                                                                      |                        | \n",
      "             |                                                        |                        |  \n",
      "             VP                                                       |                        | \n",
      "   __________|_______________                                         |                        |  \n",
      "  |                          PP                                       |                        | \n",
      "  |                 _________|________________                        |                        |  \n",
      "  |                PP                         PP                      VP                       | \n",
      "  |     ___________|___             __________|_____               ___|____                    |  \n",
      "  |    |               NP          |                NP            |        NP                  | \n",
      "  |    |      _________|_____      |     ___________|_______      |    ____|____________       |  \n",
      "Going from junior     high school  to senior       high   school  is  a  really big challenge  . \n",
      "  |    |     |         |     |     |    |           |       |     |   |    |     |      |      |  \n",
      " ...  ...   ...       ...   ...   ...  ...         ...     ...   ... ...  ...   ...    ...    ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tree import Tree #将句子改为短语图形\n",
    "s='Going from junior high school to senior high school is a really big challenge.'\n",
    "outcome=\"\"\n",
    "deparse= nlp.parse(s)\n",
    "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
    "data=deparse.split()\n",
    "print(data)\n",
    "num=0\n",
    "for n in range(0,len(data)):\n",
    "    #if data[n]=='(SBAR#@':\n",
    "    #    outcome+=\"( \"\n",
    "    #elif data[n]=='(VP' and data[n+1]!='(TO': # \n",
    "    #    outcome+=\"( \"\n",
    "    if data[n] in(\"(ADJP\",\"(ADVP\",'(NP','(VP','(PP',\"(ADJP#@\",\"(ADVP#@\",'(NP#@','(VP#@','(PP#@'):\n",
    "        outcome+=data[n]+' '\n",
    "    elif data[n]=='(ROOT#@':\n",
    "        outcome+='(Sentence#@'\n",
    "    elif \")\" not in data[n]:\n",
    "        outcome+=\"(\"\n",
    "    else:\n",
    "        outcome+=data[n]+' '\n",
    "        \n",
    "print(outcome)\n",
    "\n",
    "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
    "print(outcome)\n",
    "outcome2=''\n",
    "no=1\n",
    "for n in range(0,len(outcome)):\n",
    "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
    "        outcome2+=str(no)+'____'\n",
    "        no+=1\n",
    "    else:\n",
    "        outcome2+=outcome[n]\n",
    "tree=Tree.fromstring(outcome2)\n",
    "tree.pretty_print()\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT\r\n",
      "  (S\r\n",
      "    (S\r\n",
      "      (NP (PRP I))\r\n",
      "      (VP (VBP know)\r\n",
      "        (SBAR\r\n",
      "          (S\r\n",
      "            (NP (IN that) (JJ Chinese))\r\n",
      "            (VP (VBZ is)\r\n",
      "              (NP (DT a)\r\n",
      "                (ADJP (RB very) (JJ difficult))\r\n",
      "                (NN language)))))))\r\n",
      "    (, ,)\r\n",
      "    (CC but)\r\n",
      "    (S\r\n",
      "      (NP (PRP I))\r\n",
      "      (VP (VBP hope)\r\n",
      "        (S\r\n",
      "          (VP (TO to)\r\n",
      "            (VP (VB be)\r\n",
      "              (ADJP (JJ fluent))\r\n",
      "              (SBAR\r\n",
      "                (WHADVP (WRB when))\r\n",
      "                (S\r\n",
      "                  (NP (PRP I))\r\n",
      "                  (VP (VBP graduate)))))))))\r\n",
      "    (. .)))\n"
     ]
    }
   ],
   "source": [
    "print(nlp.parse(readsent(file)[6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import nltk\n",
      "import string\n",
      "import os\n",
      "import stanfordnlp\n",
      "\n",
      "from stanfordcorenlp import StanfordCoreNLP\n",
      "\n",
      "nlp = StanfordCoreNLP(r'C:\\Users\\spike\\python\\stanford-corenlp-full-2018-10-05',lang='en')\n",
      "\n",
      "xnlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma')\n",
      "\n",
      "xnlp = stanfordnlp.Pipeline()\n",
      "avoidance=string.punctuation+string.whitespace\n",
      "def readfile(file):\n",
      "    list_sents={}\n",
      "    name=os.path.basename(file)\n",
      "    if file[-4:]==\".txt\":\n",
      "        text=open(file,encoding='utf-8',errors='ignore')\n",
      "        avoidance=string.punctuation+string.whitespace\n",
      "        option=(\"A.\",\"B.\",\"C.\",\"D.\",\"E.\",\"F.\",\"G.\",\"B \",\"C \",\"D \",\"E \",\"F \",\"G \")\n",
      "    #print(avoidance)\n",
      "        line=''\n",
      "        for chline in text:\n",
      "            #for w in option:\n",
      "                #chline=chline.replace(w,\" \")           \n",
      "            for character in chline:\n",
      "                #if character in option and chline[chline.index(character)+1] in (\".\",\" \"):\n",
      "                    #line+=\" \"\n",
      "                if character==\"'\" or character=='\"' or character==\"’\"or character==\"“\" or character==\"”\":\n",
      "                    line+=character\n",
      "                elif character in (\"_\"):\n",
      "                    line+=\" \"\n",
      "                elif character in (\"(\",\")\",\"（\",\"）\"):\n",
      "                    line+=\" \"+character+' '\n",
      "                elif character in avoidance:\n",
      "                    line+=character\n",
      "                elif ord(character) in range(32,123):# and character not in '\\t1234567890':\n",
      "                    line+=character\n",
      "                else:\n",
      "                    line+=\"\"\n",
      "        \n",
      "        text=\"\"\n",
      "    #text=text.join(txt).replace(\"\\n\",' ')\n",
      "        text=line.replace(\"“\",'\"').replace(\"”\",'\"').replace(\"’\",\"'\").replace(\"‘\",\"'\")\n",
      "\n",
      "        return text\n",
      "    else:\n",
      "        return {}\n",
      "def lemma(s):\n",
      "    output=[]\n",
      "    doc = xnlp(s)\n",
      "    for sent in doc.sentences:\n",
      "        for word in sent.words:\n",
      "            output.append(word.lemma)\n",
      "    return output\n",
      "#doc = xnlp(\"Barack Obama was born in Hawaii.\")\n",
      "#print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
      "def gonver(s):\n",
      "    output=[]\n",
      "    doc=xnlp(s)\n",
      "    for word in doc.sentences[0].words:\n",
      "        output.append(word.governor)\n",
      "    return output\n",
      "def dependency(s):\n",
      "    s=nlp.dependency_parse(s)\n",
      "    output=[]\n",
      "    for n in range(1,len(s)+1):\n",
      "        for w in s:\n",
      "            if w[2]==n:\n",
      "                output.append(w)\n",
      "    return output\n",
      "\n",
      "#nlp.annotate(text)\n",
      "help(xnlp)\n",
      "dependency(s)\n",
      "import nltk\n",
      "import string\n",
      "import os\n",
      "import stanfordnlp\n",
      "\n",
      "from stanfordcorenlp import StanfordCoreNLP\n",
      "\n",
      "nlp = StanfordCoreNLP(r'C:\\Users\\spike\\python\\stanford-corenlp-full-2018-10-05',lang='en')\n",
      "\n",
      "xnlp = stanfordnlp.Pipeline(processors='tokenize,mwt,pos,lemma')\n",
      "\n",
      "xnlp = stanfordnlp.Pipeline()\n",
      "avoidance=string.punctuation+string.whitespace\n",
      "def readfile(file):\n",
      "    list_sents={}\n",
      "    name=os.path.basename(file)\n",
      "    if file[-4:]==\".txt\":\n",
      "        text=open(file,encoding='utf-8',errors='ignore')\n",
      "        avoidance=string.punctuation+string.whitespace\n",
      "        option=(\"A.\",\"B.\",\"C.\",\"D.\",\"E.\",\"F.\",\"G.\",\"B \",\"C \",\"D \",\"E \",\"F \",\"G \")\n",
      "    #print(avoidance)\n",
      "        line=''\n",
      "        for chline in text:\n",
      "            #for w in option:\n",
      "                #chline=chline.replace(w,\" \")           \n",
      "            for character in chline:\n",
      "                #if character in option and chline[chline.index(character)+1] in (\".\",\" \"):\n",
      "                    #line+=\" \"\n",
      "                if character==\"'\" or character=='\"' or character==\"’\"or character==\"“\" or character==\"”\":\n",
      "                    line+=character\n",
      "                elif character in (\"_\"):\n",
      "                    line+=\" \"\n",
      "                elif character in (\"(\",\")\",\"（\",\"）\"):\n",
      "                    line+=\" \"+character+' '\n",
      "                elif character in avoidance:\n",
      "                    line+=character\n",
      "                elif ord(character) in range(32,123):# and character not in '\\t1234567890':\n",
      "                    line+=character\n",
      "                else:\n",
      "                    line+=\"\"\n",
      "        \n",
      "        text=\"\"\n",
      "    #text=text.join(txt).replace(\"\\n\",' ')\n",
      "        text=line.replace(\"“\",'\"').replace(\"”\",'\"').replace(\"’\",\"'\").replace(\"‘\",\"'\")\n",
      "\n",
      "        return text\n",
      "    else:\n",
      "        return {}\n",
      "def lemma(s):\n",
      "    output=[]\n",
      "    doc = xnlp(s)\n",
      "    for sent in doc.sentences:\n",
      "        for word in sent.words:\n",
      "            output.append(word.lemma)\n",
      "    return output\n",
      "#doc = xnlp(\"Barack Obama was born in Hawaii.\")\n",
      "#print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')\n",
      "def gonver(s):\n",
      "    output=[]\n",
      "    doc=xnlp(s)\n",
      "    for word in doc.sentences[0].words:\n",
      "        output.append(word.governor)\n",
      "    return output\n",
      "def dependency(s):\n",
      "    s=nlp.dependency_parse(s)\n",
      "    output=[]\n",
      "    for n in range(1,len(s)+1):\n",
      "        for w in s:\n",
      "            if w[2]==n:\n",
      "                output.append(w)\n",
      "    return output\n",
      "\n",
      "#nlp.annotate(text)\n",
      "help(xnlp)\n",
      "def stanz(s): #分析一个句子中的各个成分包括单词、lemma、词性、依存关系、序列\n",
      "    words=nlp.word_tokenize(s)\n",
      "    dep=dependency(s)\n",
      "    tags=[]\n",
      "    for item in nlp.pos_tag(s):\n",
      "        tags.append(item[1])\n",
      "    lemmas=lemma(s)\n",
      "    gov=[]\n",
      "    dpg=[]\n",
      "    govw=[]\n",
      "    for w in dep:\n",
      "        gov.append(w[1])\n",
      "        dpg.append(w[0])\n",
      "        govw.append(words[w[1]-1])\n",
      "    length=len(words)+1\n",
      "    ifhead=[]\n",
      "    \n",
      "    for n in range(1,length): \n",
      "        if n in gov:\n",
      "            ifhead.append('head')\n",
      "        else:\n",
      "            ifhead.append('nonehead')\n",
      "        \n",
      "    tup=zip(words,tags,gov,lemmas,dpg,range(1,length),govw,ifhead)\n",
      "    output=[]\n",
      "    for item in tup:\n",
      "        output.append(item)\n",
      "    return output\n",
      "def NT_unit(s): #计算T unit 结构\n",
      "    n=0\n",
      "    #print(nlp.parse(s))\n",
      "    \n",
      "    parse=nlp.parse(s).split('\\r\\n')\n",
      "    print(parse)\n",
      "    \n",
      "    for no in range(0,len(parse)):\n",
      "        #print(line)\n",
      "        if no!=len(parse) and parse[no][-2:]=='(S' and parse[no+1][-2:]!='(S':\n",
      "            n+=1\n",
      "            print(parse[no])\n",
      "        if parse[no].replace(' ','') in ('(SBAR','(TO'):\n",
      "            n=n-1\n",
      "            #print(line)\n",
      "    return n\n",
      "def Nverb(s): #计算从句数量\n",
      "    n=0\n",
      "    #print(nlp.parse(s))\n",
      "    parse=nlp.parse(s).split()\n",
      "    #print(parse)\n",
      "    for line in parse:\n",
      "        #print(line)\n",
      "        if line in ('(VBP','(VBZ','(VBD','(MD'):\n",
      "            n+=1\n",
      "            #print(line)\n",
      "        #if line in ('(TO'):\n",
      "        #    n=n-1\n",
      "            #print(line)\n",
      "    return n\n",
      "def Ndpclause(s): #计算关系从句数量\n",
      "    n=0\n",
      "    #print(nlp.parse(s))\n",
      "    parse=nlp.parse(s).split()\n",
      "    #print(parse)\n",
      "    for line in parse:\n",
      "        #print(line)\n",
      "        if line=='(SBAR':\n",
      "            n+=1\n",
      "            #print(line)\n",
      "        #if line in ('(TO'):\n",
      "        #    n=n-1\n",
      "            #print(line)\n",
      "    return n\n",
      "def Ndpclause(s): #计算关系从句数量\n",
      "    n=0\n",
      "    #print(nlp.parse(s))\n",
      "    parse=nlp.parse(s).split()\n",
      "    #print(parse)\n",
      "    for line in parse:\n",
      "        #print(line)\n",
      "        if line=='(SBAR':\n",
      "            n+=1\n",
      "            #print(line)\n",
      "        #if line in ('(TO'):\n",
      "        #    n=n-1\n",
      "            #print(line)\n",
      "    return n\n",
      "s='I like coffee.'\n",
      "print(nlp.parse(s))\n",
      "def fragsT(s): #定位了各类括号，找到句子中的各种结构 #还是不对\n",
      "    parse=nlp.parse(s) #.replace(' ','')\n",
      "    parsepos=\"\"\n",
      "    frags=[]\n",
      "    print(parse)\n",
      "    cln=[]\n",
      "    crn=[]\n",
      "    x1=0\n",
      "    #x2=0\n",
      "    #y1=0\n",
      "    y2=0\n",
      "    x0=0\n",
      "    for n1 in range(0,len(parse)):\n",
      "        if parse[n1]=='(':\n",
      "            t=0\n",
      "            t0=0\n",
      "            while n1-t0-1>=0 and parse[n1-1-t0]==' ':\n",
      "                t0+=1\n",
      "            if parse[n1-1-t0]=='\\n' :\n",
      "                t=t0\n",
      "            else:\n",
      "                t=0\n",
      "            x1+=1\n",
      "            y1=0\n",
      "            for cha in parse[n1:]:\n",
      "                if cha==\")\":\n",
      "                    y1+=1\n",
      "            cln.append(((n1,x1-1,y1),t))\n",
      "        elif parse[n1]==\")\":\n",
      "            x2=x1\n",
      "            y2=0\n",
      "            for cha in parse[n1:]:\n",
      "                if cha==\")\":\n",
      "                    y2+=1\n",
      "            crn.append((n1,x1,y2-1))\n",
      "    #print((cln,crn))\n",
      "    #print(len(cln),len(crn))\n",
      "    for cl in cln:\n",
      "        frag=\"\"\n",
      "        n=0\n",
      "        while frag==\"\" and n<=len(crn):\n",
      "                cr=crn[n]\n",
      "                if cl[0][0]<cr[0] and  cl[0][1]-cr[2]==cr[1]-cl[0][2]:\n",
      "                    #print(((cl,cr)))\n",
      "                    frag=parse[cl[0][0]:cr[0]+1]\n",
      "                    frags.append((frag,cl[1]))\n",
      "                n+=1\n",
      "    result=[frags[0]]\n",
      "    for n in range(1,len(frags)):\n",
      "        n1=n\n",
      "        while n1>=0 and frags[n1][1]==0:\n",
      "            n1=n1-1\n",
      "        result.append((frags[n][0],frags[n1][1]))\n",
      "    return result\n",
      "\n",
      "s='I like her and like her beauty,'\n",
      "fragsT(s)\n",
      "def match(lx,ly):\n",
      "    if len(lx)>=len(ly):\n",
      "        l1=lx\n",
      "        l2=ly\n",
      "    else:\n",
      "        l1=ly\n",
      "        l2=lx\n",
      "    for n1 in range(0,len(l1)):\n",
      "        if l1[n1]==l2[0]:\n",
      "            if l1[n1:n1+len(l2)]==l2:\n",
      "                return True\n",
      "    return False\n",
      "def frags(s): #定位了各类括号，找到句子中的各种结构 #还是不对\n",
      "    parse=nlp.parse(s).replace('\\r\\n','') #.replace(' ','')\n",
      "    parsepos=\"\"\n",
      "    frags=[]\n",
      "    print(parse)\n",
      "    cln=[]\n",
      "    crn=[]\n",
      "    x1=0\n",
      "    #x2=0\n",
      "    #y1=0\n",
      "    y2=0\n",
      "    x0=0\n",
      "    for n1 in range(0,len(parse)):\n",
      "        if parse[n1]=='(':\n",
      "            x1+=1\n",
      "            y1=0\n",
      "            for cha in parse[n1:]:\n",
      "                if cha==\")\":\n",
      "                    y1+=1\n",
      "            cln.append((n1,x1-1,y1))\n",
      "        elif parse[n1]==\")\":\n",
      "            x2=x1\n",
      "            y2=0\n",
      "            for cha in parse[n1:]:\n",
      "                if cha==\")\":\n",
      "                    y2+=1\n",
      "            crn.append((n1,x1,y2-1))\n",
      "    #print((cln,crn))\n",
      "    #print(len(cln),len(crn))\n",
      "    for cl in cln:\n",
      "        frag=\"\"\n",
      "        n=0\n",
      "        while frag==\"\" and n<=len(crn):\n",
      "                cr=crn[n]\n",
      "                if cl[0]<cr[0] and  cl[1]-cr[2]==cr[1]-cl[2]:\n",
      "                    #print(((cl,cr)))\n",
      "                    frag=parse[cl[0]:cr[0]+1]\n",
      "                    frags.append(frag)\n",
      "                n+=1\n",
      "    return frags\n",
      "history\n",
      "def find_clause_component(s):\n",
      "    \n",
      "    target=frags(s)\n",
      "    #print(target)\n",
      "    result=[]\n",
      "    for c in target:\n",
      "        c=c.replace(\"(\",\" \").replace(\")\",' ')\n",
      "        c=c.split(' ')\n",
      "        templ=[]\n",
      "        for com in c:\n",
      "            if com!='':\n",
      "                templ.append(com)\n",
      "        result.append(templ)\n",
      "    return result\n",
      "def find_clause_componentT(s):\n",
      "    \n",
      "    target=fragsT(s)\n",
      "    print(target)\n",
      "    result=[]\n",
      "    for c in target:\n",
      "        com=c[0].replace(\"(\",\" \").replace(\")\",' ').replace('\\r\\n',\"\")\n",
      "        com=com.split(' ')\n",
      "        templ=[]\n",
      "        for p in com:\n",
      "            if p!='':\n",
      "                templ.append((p,c[1]))\n",
      "        result.append(templ)\n",
      "    result1=[]\n",
      "    templ1=[]\n",
      "    #print(result) \n",
      "    for t in result[0]: #[('NN', 8), ('beauty', 8)]\n",
      "        #for t in l: #('ROOT', 0)\n",
      "            print(t[0]) \n",
      "            templ1.append(t[0])\n",
      "    result1.append((templ1,(result[0][0][1],0)))\n",
      "    for n in range(1,len(result)):\n",
      "        templ=[]\n",
      "        for c in result[n]: #result[n]是个由成分和从属组成的tuple的列表\n",
      "            templ.append(c[0])\n",
      "        n1=n-1\n",
      "        #print('【check】',result[n][0][1],\",\",result[n1][0][1])\n",
      "        while n1>=0 and result[n1][0][1]>=result[n][0][1]:\n",
      "            #print('【check】',result[n][0][1],\",\",result[n1][0][1])\n",
      "            n1=n1-1\n",
      "        result1.append((templ,(result[n1][0][0],n1)))\n",
      "    return result1\n",
      "print(nlp.parse(s))\n",
      "find_clause_componentT(s)\n",
      "def find_np(s):\n",
      "    target=frags(s)\n",
      "    \n",
      "    result=[]\n",
      "    for c in target:\n",
      "        print(c)\n",
      "        c=c.replace(\"(\",\" \").replace(\")\",' ')\n",
      "        c=c.split(' ')\n",
      "        if c!='':\n",
      "            result.append(c)\n",
      "        #print(c)\n",
      "    rresult=[]\n",
      "    for c in result:\n",
      "        if c[1]=='NP':\n",
      "            rresult.append(c)\n",
      "    rrresult=[]\n",
      "    for c1 in rresult:\n",
      "        for c2 in rresult:\n",
      "            if not match(c1,c2):\n",
      "                rrresult.append(c1)\n",
      "    return rresult\n",
      "def find_np(s):\n",
      "    target=frags(s)\n",
      "    \n",
      "    result=[]\n",
      "    for c in target:\n",
      "        if c[:3]=='(NP':\n",
      "            result.append(c)\n",
      "    return result\n",
      "def find_cnp(s): #find complex np\n",
      "    target=find_np(s)\n",
      "    result=[]\n",
      "    for c1 in target:\n",
      "        n=0\n",
      "        for c2 in target:\n",
      "            if c1 in c2:\n",
      "                n+=1\n",
      "        if n<=1:\n",
      "            for c2 in (\"(PP\",'(VBN','JJ','SBAR','(S(','(VBG('):\n",
      "                if c2 in c1 and c1 not in result:\n",
      "                    result.append(c1)\n",
      "    return result\n",
      "def Ncnp(s):\n",
      "    target=find_cnp(s)\n",
      "    return len(target)\n",
      "def fdsyncomT2(s): #开始修改\n",
      "    CN=[] #子句数量\n",
      "    SBARS=[] #依存从句数量\n",
      "    RS=[] #sum of root 句子数量\n",
      "    VPS=[]\n",
      "    TN=[] #T-unit 结构 \n",
      "    CTN=[] #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS.append(target[n])\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    TN.append(target[n1])  #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                    if 'VP' not in target[n][0]:\n",
      "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "                    \n",
      "            \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ'):\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0]=='VP':\n",
      "                        #CN.append(target[n])\n",
      "                        if target[n1][0][1] in ('VBD','VBP','VBZ','MD'):\n",
      "                            CN.append(target[n])\n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ'):\n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][1][1]==n1:\n",
      "                                if target[n2][0][0]=='VP':\n",
      "                                    #SBARS.append(target[n])\n",
      "                                    if target[n2][0][1] in('VBD','VBP','VBZ','MD'):\n",
      "                                        SBARS.append(target[n])\n",
      "        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        TN.append(target[n1])\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':\n",
      "                                tempn=1\n",
      "                                n3=n2+tempn\n",
      "                                while n3<=len(target[n1][0]) and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<=len(target[n1][0]):\n",
      "                                    CTN.append(target[n1])\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN.append(target[n])\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                tempn=1\n",
      "                                n4=n3+tempn\n",
      "                                while n4<=len(target[n][0]) and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<=len(target[n][0]):\n",
      "                                    CTN.append(target[n])\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('\\n\\n')\n",
      "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
      "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
      "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
      "def fdclausecom(s): \n",
      "    SS=0\n",
      "    SBARS=0\n",
      "    RS=0\n",
      "    VPS=0\n",
      "    CTN=0\n",
      "    target=find_clause_component(s)\n",
      "    for line in target:\n",
      "        if line[1]=='ROOT':\n",
      "            RS+=1\n",
      "        if line[1]=='S' and len(line)<=2 or line[1]=='S'and line[2]!='S':\n",
      "            SS+=1\n",
      "            if 'SBAR' in line:\n",
      "                CTN+=1\n",
      "        if line[1]=='SBAR':\n",
      "            SBARS+=1\n",
      "        if line[1]=='VP':\n",
      "            VPS+=1\n",
      "    TN=SS-SBARS\n",
      "    CN=SS\n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('SENTENCE NUM:',RS)\n",
      "    print('CLAUSE NUM:',CN)\n",
      "    print('T_UNIT NUM:',TN)\n",
      "    print('DEPENDENT CLAUSE NUM:',SBARS)\n",
      "    print('COMPLEX T_UNIT NUM:',CTN)\n",
      "    print('COMPLEX NOUN PHRASE NUM:',CNN)\n",
      "    print('VERB PHRASE NUM:',VN)\n",
      "    return(RS,CN,TN,SBARS,CTN,CNN,VN)\n",
      "def fdsyncomT1(s): #开始修改\n",
      "    CN=0 #子句数量\n",
      "    SBARS=0 #依存从句数量\n",
      "    RS=0 #sum of root 句子数量\n",
      "    VPS=0\n",
      "    TN=0 #T-unit 结构 \n",
      "    CTN=0 #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS+=1\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    TN+=1  #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                    if 'VP' not in target[n][0]:\n",
      "                        CN+=1  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "                    \n",
      "            \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ'):\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('VBD','VBP','VBZ'):\n",
      "                        CN+=1\n",
      "                    if target[n1][0][0]=='MD' and target[n1][0][1]=='VP':\n",
      "                        CN+=1\n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ'):\n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][1][1]==n1:\n",
      "                                if target[n2][0][0] in ('VBD','VBP','VBZ'):\n",
      "                                    SBARS+=1\n",
      "                                if target[n2][0][:2]==('VP','MD'):\n",
      "                                    SBARS+=1\n",
      "        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        TN+=1\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':\n",
      "                                tempn=1\n",
      "                                n3=n2+tempn\n",
      "                                while n3<=len(target[n1][0]) and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<=len(target[n1][0]):\n",
      "                                    CTN+=1\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN+=1\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                tempn=1\n",
      "                                n4=n3+tempn\n",
      "                                while n4<=len(target[n][0]) and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<=len(target[n][0]):\n",
      "                                    CTN+=1\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    \n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(RS,CN,TN,SBARS,CTN,CNN,VN)\n",
      "def fdsyncomT3(s): #暂时完成！！\n",
      "    CN=[] #子句数量\n",
      "    SBARS=[] #依存从句数量\n",
      "    RS=[] #sum of root 句子数量\n",
      "    VPS=[]\n",
      "    TN=[] #T-unit 结构 \n",
      "    CTN=[] #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS.append(target[n])\n",
      "            for n1 in range(n+1,len(target)-1):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    if 'VP' in target[n][0]:\n",
      "                        TN.append(target[n1])  #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
      "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    CTN.append(target[n1])\n",
      "                    else:\n",
      "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "                    \n",
      "            \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='VP': \n",
      "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
      "                        #CN.append(target[n])\n",
      "                        n2=1\n",
      "                        while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
      "                            print('【check】',n2,target[n1],target[n1][0][n2])\n",
      "                            n2+=1\n",
      "                        if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                            if target[n] not in CN:\n",
      "                                CN.append(target[n])\n",
      "\n",
      "                    else:                  #VP间接从属于 S不能计算入内\n",
      "                        for n3 in range(1,n1-1): \n",
      "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
      "                                n2=1\n",
      "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                    n2+=1\n",
      "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in CN:\n",
      "                                        CN=CN\n",
      "                                        #CN.append(target[n])\n",
      "                                \n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][0][0]=='VP': #VP直接从属于 S \n",
      "                                if target[n2][1][1]==n1:\n",
      "                                    n3=1\n",
      "                                    while n3<=len(target[n2][0])-1 and target[n2][0][n3] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                        n3+=1\n",
      "                                    if n3<len(target[n2][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                        if target[n] not in SBARS:\n",
      "                                            SBARS.append(target[n])\n",
      "                                else:                  #VP间接从属于 S\n",
      "                                    for n4 in range(1,n2-1): \n",
      "                                        if target[n2][1][1]==n4 and target[n4][1][1]==n1:\n",
      "                                            n5=1\n",
      "                                            while n5<=len(target[n2][0])-1 and target[n2][0][n5] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                                n5+=1\n",
      "                                            if n5<len(target[n2][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                                if target[n] not in SBARS:\n",
      "                                                    SBARS.append(target[n])\n",
      "                                        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        if target[n1] not in TN:\n",
      "                            TN.append(target[n1])\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    if target[n1] not in CTN:\n",
      "                                        CTN.append(target[n1])\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN.append(target[n])\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                n4=n3+1\n",
      "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<len(target[n][0]):\n",
      "                                    CTN.append(target[n])\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('\\n\\n')\n",
      "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
      "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
      "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
      "def readsent(file):\n",
      "    list_sents=[]\n",
      "    \n",
      "    text=open(file,encoding='utf-8',errors='ignore')\n",
      "        \n",
      "    for line in text:\n",
      "        line=line.strip()\n",
      "        sent=nltk.sent_tokenize(line)\n",
      "        \n",
      "        list_sents+=sent\n",
      "    return list_sents\n",
      "def SenComAnalyser3(file):\n",
      "    target=readsent(file)\n",
      "    data=[]\n",
      "    sumWN=[]\n",
      "    result=''\n",
      "    datatitle=['W','S','C','T','DC','CT','CN','VP']\n",
      "    headline='SENT\\COMP'\n",
      "    for t in datatitle:\n",
      "        headline+='\\t'+t\n",
      "    result+=headline+'\\n'\n",
      "    print(headline)\n",
      "    n=1\n",
      "    for line in target:\n",
      "        WN=len(nltk.word_tokenize(line))\n",
      "        l=''\n",
      "        l+='sentence'+str(n)+'\\t'\n",
      "        l+=str(WN)+'\\t'\n",
      "        for d in fdclausecom(line):\n",
      "            l+=str(d)+'\\t'\n",
      "        result+=l+'\\n'\n",
      "        n+=1\n",
      "        #print('sentence'+str(n),'\\t',l)\n",
      "    print(result)\n",
      "def SenComAnalyser4(file):\n",
      "    target=readsent(file)\n",
      "    data=[]\n",
      "    sumWN=[]\n",
      "    result=''\n",
      "    datatitle=['W','S','C','T','DC','CT','CN','VP']\n",
      "    headline='SENT\\COMP'\n",
      "    for t in datatitle:\n",
      "        headline+='\\t'+t\n",
      "    result+=headline+'\\n'\n",
      "    print(headline)\n",
      "    n=1\n",
      "    for line in target:\n",
      "        WN=len(nltk.word_tokenize(line))\n",
      "        l=''\n",
      "        l+='sentence'+str(n)+'\\t'\n",
      "        l+=str(WN)+'\\t'\n",
      "        for d in fdsyncomT3(line):\n",
      "            l+=str(d)+'\\t'\n",
      "        result+=l+'\\n'\n",
      "        n+=1\n",
      "        #print('sentence'+str(n),'\\t',l)\n",
      "    print(result)\n",
      "readsent(file)[4]\n",
      "print(nlp.parse(readsent(file)[4]))\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "readsent(file)[4]\n",
      "print(nlp.parse(readsent(file)[4]))\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "readsent(file)[6]\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "readsent(file)[6]\n",
      "SenComAnalyser3(file)\n",
      "def SenComAnalyser1(file):\n",
      "    target=readsent(file)\n",
      "    data=[]\n",
      "    sumWN=[]\n",
      "    for line in target:\n",
      "        WN=len(nltk.word_tokenize(line))\n",
      "        sumWN.append(WN)\n",
      "        data.append(fdclausecom(line))\n",
      "    \n",
      "    S=''\n",
      "    T=''\n",
      "    C=\"\"\n",
      "    CT=\"\"\n",
      "    DC=\"\"\n",
      "    CN=\"\"\n",
      "    VP=\"\"\n",
      "    datas=[S,C,T,DC,CT,CN,VP]\n",
      "    datatitle=['W','S','C','T','DC','CT','CN','VP']\n",
      "    n=0\n",
      "    sumresult=[]\n",
      "    W=''\n",
      "    for c in sumWN:\n",
      "         W+=str(c)+'\\t'\n",
      "    sumresult.append(W)\n",
      "    for l in datas:\n",
      "        for c in data:\n",
      "            #print(c)\n",
      "            l+=str(c[n])+'\\t'\n",
      "        #print(l)\n",
      "        sumresult.append(l)\n",
      "        n+=1\n",
      "    headline='components'\n",
      "    for n in range(1,len(target)+1):\n",
      "        headline+='\\t'+'sentence'+str(n)\n",
      "    print(headline)\n",
      "    for n in range(0,len(sumresult)):\n",
      "        print(datatitle[n],'\\t',sumresult[n])\n",
      "def SenComAnalyser2(file):\n",
      "    target=readsent(file)\n",
      "    data=[]\n",
      "    sumWN=[]\n",
      "    result=''\n",
      "    datatitle=[\"W\",\"W/T\",\"W/C\",\"C\",\"C/T\",\"CT/T\",\"DC/C\",\"DC/T\",\"T\",'CN',\"CN/C\",\"CN/T\",\"VP/C\"]\n",
      "    headline='\\t长度特征 \\t \\t \\t子句数量 \\t从属情况 \\t \\t \\t \\t并列情况 \\t特殊结构占比\\t \\t \\t'\n",
      "    result+=headline+'\\n'\n",
      "    headline='SENT\\COMP'\n",
      "    for t in datatitle:\n",
      "        headline+='\\t'+t\n",
      "    result+=headline+'\\n'\n",
      "    print(headline)\n",
      "    n=1\n",
      "    \n",
      "        \n",
      "    for line in target:\n",
      "        w=len(nltk.word_tokenize(line))\n",
      "        l=''\n",
      "        l+='sentence'+str(n)+'\\t'\n",
      "        l+=str(w)+'\\t'   #W\n",
      "        data=fdclausecom(line)\n",
      "        if data[2] !=0 :       #W/T\n",
      "            l+=str(round(w/data[2],2))  +'\\t' \n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :  #W/C\n",
      "            l+=str(round(w/data[1],2))  +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[1])+'\\t'   #C\n",
      "\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[1]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[4]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[3]/data[1],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[3]/data[2],2))   +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[2])+'\\t'\n",
      "        l+=str(data[5])+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[5]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[5]/data[2],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[6]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        result+=l+'\\n'\n",
      "        n+=1\n",
      "        #print('sentence'+str(n),'\\t',l)\n",
      "    print(result)\n",
      "def SenComAnalyser5(file):\n",
      "    target=readsent(file)\n",
      "    data=[]\n",
      "    sumWN=[]\n",
      "    result=''\n",
      "    datatitle=[\"W\",\"W/T\",\"W/C\",\"C\",\"C/T\",\"CT/T\",\"DC/C\",\"DC/T\",\"T\",'CN',\"CN/C\",\"CN/T\",\"VP/C\"]\n",
      "    headline='\\t长度特征 \\t \\t \\t子句数量 \\t从属情况 \\t \\t \\t \\t并列情况 \\t特殊结构占比\\t \\t \\t'\n",
      "    result+=headline+'\\n'\n",
      "    headline='SENT\\COMP'\n",
      "    for t in datatitle:\n",
      "        headline+='\\t'+t\n",
      "    result+=headline+'\\n'\n",
      "    print(headline)\n",
      "    n=1\n",
      "    \n",
      "        \n",
      "    for line in target:\n",
      "        w=len(nltk.word_tokenize(line))\n",
      "        l=''\n",
      "        l+='sentence'+str(n)+'\\t'\n",
      "        l+=str(w)+'\\t'   #W\n",
      "        data=fdsyncomT3(line)\n",
      "        if data[2] !=0 :       #W/T\n",
      "            l+=str(round(w/data[2],2))  +'\\t' \n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :  #W/C\n",
      "            l+=str(round(w/data[1],2))  +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[1])+'\\t'   #C\n",
      "\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[1]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[4]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[3]/data[1],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[3]/data[2],2))   +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[2])+'\\t'\n",
      "        l+=str(data[5])+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[5]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[5]/data[2],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[6]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        result+=l+'\\n'\n",
      "        n+=1\n",
      "        #print('sentence'+str(n),'\\t',l)\n",
      "    print(result)\n",
      "readsent(file)[2]\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "SenComAnalyser5(file)\n",
      "s=\"My name is Adam and I'm a freshman at senior high school. \"\n",
      "fdclausecom(s)\n",
      "from nltk.tree import Tree\n",
      "s=\"What is past is the prologue and I like it. \"\n",
      "#s=text\n",
      "deparse= nlp.parse(s)\n",
      "tree=Tree.fromstring(deparse)\n",
      "tree.draw()\n",
      "def stanz_text(txt): #将段落转换为数据表\n",
      "    output=[]\n",
      "    txt=nltk.sent_tokenize(txt)\n",
      "    for sent in txt:\n",
      "        output.append(stanz(sent))\n",
      "    prtline=''\n",
      "    text=''\n",
      "    for line in output:\n",
      "        #prtline+=\"\\n\"\n",
      "        for item in line:\n",
      "            if item[0] in string.punctuation:\n",
      "                text+=item[0]\n",
      "            else:\n",
      "                text+=' '+item[0]\n",
      "            for inf in item:\n",
      "                prtline+=str(inf)+'\\t'\n",
      "            prtline+='\\n'\n",
      "        prtline+=50*'-'+'\\n'\n",
      "    print(prtline)\n",
      "    print(text[1:])\n",
      "    return output\n",
      "#s = 'Unable to resist the impulse of curiosity, she raised her eyes to his.' #stanford NLP 示范\n",
      "#doc=xnlp(s)\n",
      "#print (lemma(s))\n",
      "#print(gonver(s))\n",
      "#print(*[f\"index: {word.index.rjust(2)}\\tword: {word.text.ljust(11)}\\tgovernor index: {word.governor}\\tgovernor: {(doc.sentences[0].words[word.governor-1].text if word.governor > 0 else 'root').ljust(11)}\\tdeprel: {word.dependency_relation}\" for word in doc.sentences[0].words], sep='\\n')\n",
      "from nltk.tree import Tree #将句子改为填图练习\n",
      "s=\"What is past is the prologue and I like it. \"\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(s)\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)):\n",
      "    if n<=len(deparse)-2 and deparse[n]+deparse[n+1]==\"NP\" or n-1>=0 and deparse[n-1]+deparse[n]==\"NP\" or\\\n",
      "        n-3>=0 and deparse[n-3]+deparse[n-2]+deparse[n-1]+deparse[n] in ('ADJP',\"ADVP\") or\\\n",
      "        n-2>=0 and n<=len(deparse)-2 and deparse[n-2]+deparse[n-1]+deparse[n]+deparse[n+1] in ('ADJP',\"ADVP\") or\\\n",
      "        n-1>=0 and n<=len(deparse)-3 and deparse[n-1]+deparse[n]+deparse[n+1]+deparse[n+2] in ('ADJP',\"ADVP\") or\\\n",
      "        n<=len(deparse)-4 and deparse[n]+deparse[n+1]+deparse[n+2]+deparse[n+3] in ('ADJP',\"ADVP\"):\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "readsent(file)[6]\n",
      "readsent(file)[6]\n",
      "fdsyncom5(readsentfile[6])\n",
      "readsent(file)[6]\n",
      "fdsyncom3(readsentfile[6])\n",
      "readsent(file)[6]\n",
      "fdsyncomT3(readsentfile[6])\n",
      "readsent(file)[6]\n",
      "fdsyncomT3(readsentfile[6])\n",
      "fdsyncomT3(readsent(file)[6])\n",
      "def fdsyncomT4(s): #暂时完成！！\n",
      "    CN=[] #子句数量\n",
      "    SBARS=[] #依存从句数量\n",
      "    RS=[] #sum of root 句子数量\n",
      "    VPS=[]\n",
      "    TN=[] #T-unit 结构 \n",
      "    CTN=[] #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS.append(target[n])\n",
      "            for n1 in range(n+1,len(target)-1):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    if 'VP' in target[n][0]:\n",
      "                        TN.append(target[n1])  #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
      "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    CTN.append(target[n1])\n",
      "                    else:\n",
      "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='VP': \n",
      "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
      "                        #CN.append(target[n])\n",
      "                        n3=0\n",
      "                        for n2 in range(0,len(target)-1):\n",
      "                            if n2<=len(target)-1 and target[n2][0][0] in ('VBD','VBP','VBZ','MD') and target[n2][1]==('VP',n1):\n",
      "                                n3+=1\n",
      "                        if n3>0:\n",
      "                            CN.append(target[n])  #VP下面直接存在 VBD VBP VBZ 和 MD \n",
      "\n",
      "                    else:                  #VP间接从属于 S不能计算入内\n",
      "                        for n3 in range(1,n1-1): \n",
      "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
      "                                n2=1\n",
      "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                    n2+=1\n",
      "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in CN:\n",
      "                                        CN=CN\n",
      "                                        #CN.append(target[n])\n",
      "                                \n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][0][0]=='VP': #VP直接从属于 S \n",
      "                                tempn=0\n",
      "                                for n3 in range(0,len(target)-1):\n",
      "                                    if n3<=len(target)-1 and target[n3][0][0] in ('VBD','VBP','VBZ','MD') and target[n3][1]==('VP',n2):\n",
      "                                        tempn+=1\n",
      "                                if tempn>0:  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in SBARS:\n",
      "                                        SBARS.append(target[n])\n",
      "                                #不考虑VP间接从属于 S\n",
      "\n",
      "                                        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        if target[n1] not in TN:\n",
      "                            TN.append(target[n1])\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    if target[n1] not in CTN:\n",
      "                                        CTN.append(target[n1])\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN.append(target[n])\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                n4=n3+1\n",
      "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<len(target[n][0]):\n",
      "                                    CTN.append(target[n])\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('\\n\\n')\n",
      "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
      "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
      "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "readsent(file)[6]\n",
      "fdsyncomT4(readsent(file)[6])\n",
      "def fdsyncomT4(s): #暂时完成！！\n",
      "    CN=[] #子句数量\n",
      "    SBARS=[] #依存从句数量\n",
      "    RS=[] #sum of root 句子数量\n",
      "    VPS=[]\n",
      "    TN=[] #T-unit 结构 \n",
      "    CTN=[] #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS.append(target[n])\n",
      "            for n1 in range(n+1,len(target)-1):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    if 'VP' in target[n][0]:\n",
      "                        TN.append(target[n1])  \n",
      "                        #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
      "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    CTN.append(target[n1])\n",
      "                    else:\n",
      "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='VP': \n",
      "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
      "                        #CN.append(target[n])\n",
      "                        n3=0\n",
      "                        for n2 in range(0,len(target)-1):\n",
      "                            if target[n2][0][0] in ('VBD','VBP','VBZ','MD') and target[n2][1]==('VP',n1):\n",
      "                                n3+=1\n",
      "                        if n3>0 and target[n] not in CN:\n",
      "                            CN.append(target[n])  #VP下面直接存在 VBD VBP VBZ 和 MD \n",
      "\n",
      "                    else:                  #VP间接从属于 S不能计算入内\n",
      "                        for n3 in range(1,n1-1): \n",
      "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
      "                                n2=1\n",
      "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                    n2+=1\n",
      "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in CN:\n",
      "                                        CN=CN\n",
      "                                        #CN.append(target[n])\n",
      "                                \n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][0][0]=='VP': #VP直接从属于 S \n",
      "                                tempn=0\n",
      "                                for n3 in range(0,len(target)-1):\n",
      "                                    if n3<=len(target)-1 and target[n3][0][0] in ('VBD','VBP','VBZ','MD') and target[n3][1]==('VP',n2):\n",
      "                                        tempn+=1\n",
      "                                if tempn>0:  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in SBARS:\n",
      "                                        SBARS.append(target[n])\n",
      "                                #不考虑VP间接从属于 S\n",
      "\n",
      "                                        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        if target[n1] not in TN:\n",
      "                            TN.append(target[n1])\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    if target[n1] not in CTN:\n",
      "                                        CTN.append(target[n1])\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN.append(target[n])\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                n4=n3+1\n",
      "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<len(target[n][0]):\n",
      "                                    CTN.append(target[n])\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('\\n\\n')\n",
      "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
      "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
      "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "readsent(file)[6]\n",
      "fdsyncomT4(readsent(file)[6])\n",
      "find_clause_componentT(s)\n",
      "def fdsyncomT4(s): #暂时完成！！\n",
      "    CN=[] #子句数量\n",
      "    SBARS=[] #依存从句数量\n",
      "    RS=[] #sum of root 句子数量\n",
      "    VPS=[]\n",
      "    TN=[] #T-unit 结构 \n",
      "    CTN=[] #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS.append(target[n])\n",
      "            for n1 in range(n+1,len(target)-1):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    if 'VP' in target[n][0]:\n",
      "                        TN.append(target[n1])  \n",
      "                        #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
      "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    CTN.append(target[n1])\n",
      "                    else:\n",
      "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='VP': \n",
      "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
      "                        #CN.append(target[n])\n",
      "                        n3=0\n",
      "                        for n2 in range(0,len(target)-1):\n",
      "                            if target[n2][0][0] in ('VBD','VBP','VBZ','MD') and target[n2][1][1]==(n1:\n",
      "                                n3+=1\n",
      "                        if n3>0 and target[n] not in CN:\n",
      "                            CN.append(target[n])  #VP下面直接存在 VBD VBP VBZ 和 MD \n",
      "\n",
      "                    else:                  #VP间接从属于 S不能计算入内\n",
      "                        for n3 in range(1,n1-1): \n",
      "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
      "                                n2=1\n",
      "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                    n2+=1\n",
      "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in CN:\n",
      "                                        CN=CN\n",
      "                                        #CN.append(target[n])\n",
      "                                \n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][0][0]=='VP': #VP直接从属于 S \n",
      "                                tempn=0\n",
      "                                for n3 in range(0,len(target)-1):\n",
      "                                    if n3<=len(target)-1 and target[n3][0][0] in ('VBD','VBP','VBZ','MD') and target[n3][1][1]==n2:\n",
      "                                        tempn+=1\n",
      "                                if tempn>0:  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in SBARS:\n",
      "                                        SBARS.append(target[n])\n",
      "                                #不考虑VP间接从属于 S\n",
      "\n",
      "                                        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        if target[n1] not in TN:\n",
      "                            TN.append(target[n1])\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    if target[n1] not in CTN:\n",
      "                                        CTN.append(target[n1])\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN.append(target[n])\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                n4=n3+1\n",
      "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<len(target[n][0]):\n",
      "                                    CTN.append(target[n])\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('\\n\\n')\n",
      "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
      "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
      "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
      "def fdsyncomT4(s): #暂时完成！！\n",
      "    CN=[] #子句数量\n",
      "    SBARS=[] #依存从句数量\n",
      "    RS=[] #sum of root 句子数量\n",
      "    VPS=[]\n",
      "    TN=[] #T-unit 结构 \n",
      "    CTN=[] #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS.append(target[n])\n",
      "            for n1 in range(n+1,len(target)-1):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    if 'VP' in target[n][0]:\n",
      "                        TN.append(target[n1])  \n",
      "                        #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
      "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    CTN.append(target[n1])\n",
      "                    else:\n",
      "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='VP': \n",
      "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
      "                        #CN.append(target[n])\n",
      "                        n3=0\n",
      "                        for n2 in range(0,len(target)-1):\n",
      "                            if target[n2][0][0] in ('VBD','VBP','VBZ','MD') and target[n2][1][1]==n1:\n",
      "                                n3+=1\n",
      "                        if n3>0 and target[n] not in CN:\n",
      "                            CN.append(target[n])  #VP下面直接存在 VBD VBP VBZ 和 MD \n",
      "\n",
      "                    else:                  #VP间接从属于 S不能计算入内\n",
      "                        for n3 in range(1,n1-1): \n",
      "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
      "                                n2=1\n",
      "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                    n2+=1\n",
      "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in CN:\n",
      "                                        CN=CN\n",
      "                                        #CN.append(target[n])\n",
      "                                \n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][0][0]=='VP': #VP直接从属于 S \n",
      "                                tempn=0\n",
      "                                for n3 in range(0,len(target)-1):\n",
      "                                    if n3<=len(target)-1 and target[n3][0][0] in ('VBD','VBP','VBZ','MD') and target[n3][1][1]==n2:\n",
      "                                        tempn+=1\n",
      "                                if tempn>0:  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in SBARS:\n",
      "                                        SBARS.append(target[n])\n",
      "                                #不考虑VP间接从属于 S\n",
      "\n",
      "                                        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        if target[n1] not in TN:\n",
      "                            TN.append(target[n1])\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    if target[n1] not in CTN:\n",
      "                                        CTN.append(target[n1])\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN.append(target[n])\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                n4=n3+1\n",
      "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<len(target[n][0]):\n",
      "                                    CTN.append(target[n])\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('\\n\\n')\n",
      "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
      "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
      "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "readsent(file)[6]\n",
      "fdsyncomT4(readsent(file)[6])\n",
      "def fdsyncomT4(s): #暂时完成！！\n",
      "    CN=[] #子句数量\n",
      "    SBARS=[] #依存从句数量\n",
      "    RS=[] #sum of root 句子数量\n",
      "    VPS=[]\n",
      "    TN=[] #T-unit 结构 \n",
      "    CTN=[] #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS.append(target[n])\n",
      "            for n1 in range(n+1,len(target)-1):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    if 'VP' in target[n][0]:\n",
      "                        TN.append(target[n1])  \n",
      "                        #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
      "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    CTN.append(target[n1])\n",
      "                    else:\n",
      "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='VP': \n",
      "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
      "                        #CN.append(target[n])\n",
      "                        n3=0\n",
      "                        for n2 in range(0,len(target)-1):\n",
      "                            if target[n2][0][0] in ('VBD','VBP','VBZ','MD') and target[n2][1][1]==n1:\n",
      "                                n3+=1\n",
      "                        if n3>0 and target[n] not in CN:\n",
      "                            CN.append(target[n])  #VP下面直接存在 VBD VBP VBZ 和 MD \n",
      "\n",
      "                    else:                  #VP间接从属于 S不能计算入内\n",
      "                        for n3 in range(1,n1-1): \n",
      "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
      "                                n2=1\n",
      "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                    n2+=1\n",
      "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in CN:\n",
      "                                        CN=CN\n",
      "                                        #CN.append(target[n])\n",
      "                                \n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][0][0]=='VP': #VP直接从属于 S \n",
      "                                tempn=0\n",
      "                                for n3 in range(0,len(target)-1):\n",
      "                                    if n3<=len(target)-1 and target[n3][0][0] in ('VBD','VBP','VBZ','MD') and target[n3][1][1]==n2:\n",
      "                                        tempn+=1\n",
      "                                if tempn>0:  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in SBARS:\n",
      "                                        SBARS.append(target[n])\n",
      "                                #不考虑VP间接从属于 S\n",
      "\n",
      "                                        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        if target[n1] not in TN:\n",
      "                            TN.append(target[n1])\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    if target[n1] not in CTN:\n",
      "                                        CTN.append(target[n1])\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN.append(target[n])\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                n4=n3+1\n",
      "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<len(target[n][0]):\n",
      "                                    CTN.append(target[n])\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('\\n\\n')\n",
      "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
      "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
      "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
      "def fdsyncomT4(s): #暂时完成！！\n",
      "    CN=[] #子句数量\n",
      "    SBARS=[] #依存从句数量\n",
      "    RS=[] #sum of root 句子数量\n",
      "    VPS=[]\n",
      "    TN=[] #T-unit 结构 \n",
      "    CTN=[] #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS.append(target[n])\n",
      "            for n1 in range(n+1,len(target)-1):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    if 'VP' in target[n][0]:\n",
      "                        TN.append(target[n1])  \n",
      "                        #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
      "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    CTN.append(target[n1])\n",
      "                    else:\n",
      "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='VP': \n",
      "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
      "                        #CN.append(target[n])\n",
      "                        n3=0\n",
      "                        for n2 in range(0,len(target)-1):\n",
      "                            print(target[n2])\n",
      "                            if target[n2][0][0] in ('VBD','VBP','VBZ','MD') and target[n2][1][1]==n1:\n",
      "                                n3+=1\n",
      "                        if n3>0 and target[n] not in CN:\n",
      "                            CN.append(target[n])  #VP下面直接存在 VBD VBP VBZ 和 MD \n",
      "\n",
      "                    else:                  #VP间接从属于 S不能计算入内\n",
      "                        for n3 in range(1,n1-1): \n",
      "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
      "                                n2=1\n",
      "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                    n2+=1\n",
      "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in CN:\n",
      "                                        CN=CN\n",
      "                                        #CN.append(target[n])\n",
      "                                \n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][0][0]=='VP': #VP直接从属于 S \n",
      "                                tempn=0\n",
      "                                for n3 in range(0,len(target)-1):\n",
      "                                    if n3<=len(target)-1 and target[n3][0][0] in ('VBD','VBP','VBZ','MD') and target[n3][1][1]==n2:\n",
      "                                        tempn+=1\n",
      "                                if tempn>0:  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in SBARS:\n",
      "                                        SBARS.append(target[n])\n",
      "                                #不考虑VP间接从属于 S\n",
      "\n",
      "                                        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        if target[n1] not in TN:\n",
      "                            TN.append(target[n1])\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    if target[n1] not in CTN:\n",
      "                                        CTN.append(target[n1])\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN.append(target[n])\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                n4=n3+1\n",
      "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<len(target[n][0]):\n",
      "                                    CTN.append(target[n])\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('\\n\\n')\n",
      "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
      "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
      "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "readsent(file)[6]\n",
      "fdsyncomT4(readsent(file)[6])\n",
      "def fdsyncomT4(s): #暂时完成！！\n",
      "    CN=[] #子句数量\n",
      "    SBARS=[] #依存从句数量\n",
      "    RS=[] #sum of root 句子数量\n",
      "    VPS=[]\n",
      "    TN=[] #T-unit 结构 \n",
      "    CTN=[] #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS.append(target[n])\n",
      "            for n1 in range(n+1,len(target)-1):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    if 'VP' in target[n][0]:\n",
      "                        TN.append(target[n1])  \n",
      "                        #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
      "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    CTN.append(target[n1])\n",
      "                    else:\n",
      "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='VP' and target[n1][0][1] in ('VBD','VBP','VBZ','MD'):\n",
      "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
      "                        if target[n] not in CN:\n",
      "                            CN.append(target[n])  #VP下面直接存在 VBD VBP VBZ 和 MD \n",
      "\n",
      "                    else:                  #VP间接从属于 S不能计算入内\n",
      "                        for n3 in range(1,n1-1): \n",
      "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
      "                                n2=1\n",
      "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                    n2+=1\n",
      "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in CN:\n",
      "                                        CN=CN\n",
      "                                        #CN.append(target[n])\n",
      "                                \n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][0][0]=='VP': #VP直接从属于 S \n",
      "                                tempn=0\n",
      "                                for n3 in range(0,len(target)-1):\n",
      "                                    if n3<=len(target)-1 and target[n3][0][0] in ('VBD','VBP','VBZ','MD') and target[n3][1][1]==n2:\n",
      "                                        tempn+=1\n",
      "                                if tempn>0:  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in SBARS:\n",
      "                                        SBARS.append(target[n])\n",
      "                                #不考虑VP间接从属于 S\n",
      "\n",
      "                                        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        if target[n1] not in TN:\n",
      "                            TN.append(target[n1])\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    if target[n1] not in CTN:\n",
      "                                        CTN.append(target[n1])\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN.append(target[n])\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                n4=n3+1\n",
      "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<len(target[n][0]):\n",
      "                                    CTN.append(target[n])\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('\\n\\n')\n",
      "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
      "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
      "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
      "find_clause_componentT(s)\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "readsent(file)[6]\n",
      "fdsyncomT4(readsent(file)[6])\n",
      "def fdsyncomT4(s): #暂时完成！！\n",
      "    CN=[] #子句数量\n",
      "    SBARS=[] #依存从句数量\n",
      "    RS=[] #sum of root 句子数量\n",
      "    VPS=[]\n",
      "    TN=[] #T-unit 结构 \n",
      "    CTN=[] #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS.append(target[n])\n",
      "            for n1 in range(n+1,len(target)-1):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    if 'VP' in target[n][0]:\n",
      "                        TN.append(target[n1])  \n",
      "                        #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
      "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    CTN.append(target[n1])\n",
      "                    else:\n",
      "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='VP' and target[n1][0][1] in ('VBD','VBP','VBZ','MD'):\n",
      "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
      "                        if target[n] not in CN:\n",
      "                            CN.append(target[n])  #VP下面直接存在 VBD VBP VBZ 和 MD \n",
      "\n",
      "                    else:                  #VP间接从属于 S不能计算入内\n",
      "                        for n3 in range(1,n1-1): \n",
      "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
      "                                n2=1\n",
      "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                    n2+=1\n",
      "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in CN:\n",
      "                                        CN=CN\n",
      "                                        #CN.append(target[n])\n",
      "                                \n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][0][0]=='VP' and target[n2][0][1] in ('VBD','VBP','VBZ','MD'): #VP直接从属于 S \n",
      "                                    if target[n] not in SBARS:\n",
      "                                        SBARS.append(target[n])\n",
      "                                #不考虑VP间接从属于 S\n",
      "\n",
      "                                        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        if target[n1] not in TN:\n",
      "                            TN.append(target[n1])\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    if target[n1] not in CTN:\n",
      "                                        CTN.append(target[n1])\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN.append(target[n])\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                n4=n3+1\n",
      "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<len(target[n][0]):\n",
      "                                    CTN.append(target[n])\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('\\n\\n')\n",
      "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
      "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
      "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "readsent(file)[6]\n",
      "fdsyncomT4(readsent(file)[6])\n",
      "def SenComAnalyser5(file):\n",
      "    target=readsent(file)\n",
      "    data=[]\n",
      "    sumWN=[]\n",
      "    result=''\n",
      "    datatitle=[\"W\",\"W/T\",\"W/C\",\"C\",\"C/T\",\"CT/T\",\"DC/C\",\"DC/T\",\"T\",'CN',\"CN/C\",\"CN/T\",\"VP/C\"]\n",
      "    headline='\\t长度特征 \\t \\t \\t子句数量 \\t从属情况 \\t \\t \\t \\t并列情况 \\t特殊结构占比\\t \\t \\t'\n",
      "    result+=headline+'\\n'\n",
      "    headline='SENT\\COMP'\n",
      "    for t in datatitle:\n",
      "        headline+='\\t'+t\n",
      "    result+=headline+'\\n'\n",
      "    print(headline)\n",
      "    n=1\n",
      "    \n",
      "        \n",
      "    for line in target:\n",
      "        w=len(nltk.word_tokenize(line))\n",
      "        l=''\n",
      "        l+='sentence'+str(n)+'\\t'\n",
      "        l+=str(w)+'\\t'   #W\n",
      "        data=fdsyncomT4(line)\n",
      "        if data[2] !=0 :       #W/T\n",
      "            l+=str(round(w/data[2],2))  +'\\t' \n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :  #W/C\n",
      "            l+=str(round(w/data[1],2))  +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[1])+'\\t'   #C\n",
      "\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[1]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[4]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[3]/data[1],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[3]/data[2],2))   +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[2])+'\\t'\n",
      "        l+=str(data[5])+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[5]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[5]/data[2],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[6]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        result+=l+'\\n'\n",
      "        n+=1\n",
      "        #print('sentence'+str(n),'\\t',l)\n",
      "    print(result)\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "SenComAnalyser4(file)\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "SenComAnalyser5(file)\n",
      "from nltk.tree import Tree #将句子改为填图练习\n",
      "s=\"What is past is the prologue and I like it. \"\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[2])\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)):\n",
      "    if n<=len(deparse)-2 and deparse[n]+deparse[n+1]==\"NP\" or n-1>=0 and deparse[n-1]+deparse[n]==\"NP\" or\\\n",
      "        n-3>=0 and deparse[n-3]+deparse[n-2]+deparse[n-1]+deparse[n] in ('ADJP',\"ADVP\") or\\\n",
      "        n-2>=0 and n<=len(deparse)-2 and deparse[n-2]+deparse[n-1]+deparse[n]+deparse[n+1] in ('ADJP',\"ADVP\") or\\\n",
      "        n-1>=0 and n<=len(deparse)-3 and deparse[n-1]+deparse[n]+deparse[n+1]+deparse[n+2] in ('ADJP',\"ADVP\") or\\\n",
      "        n<=len(deparse)-4 and deparse[n]+deparse[n+1]+deparse[n+2]+deparse[n+3] in ('ADJP',\"ADVP\"):\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree\n",
      "deparse= nlp.parse(readsent(file)[2])\n",
      "tree=Tree.fromstring(deparse)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree\n",
      "s=readsent(file)[6]\n",
      "#s=text\n",
      "deparse= nlp.parse(s)\n",
      "tree=Tree.fromstring(deparse)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)-1):\n",
      "    if n>-1 and n<=len(deparse)-3 and deparse[n：n+2]==\"SBAR\" or\\\n",
      "        n>0 and n<=len(deparse)-2 and deparse[n-1:n+1]==\"SBAR\" or\\\n",
      "        n>1 and n<=len(deparse) and deparse[n-2:n]=='SBAR' or\\\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)-1):\n",
      "    if n>-1 and n<=len(deparse)-3 and deparse[n：n+2]==\"SBAR\" or\\\n",
      "        n>0 and n<=len(deparse)-2 and deparse[n-1:n+1]==\"SBAR\" or\\\n",
      "        n>1 and n<=len(deparse) and deparse[n-2:n]=='SBAR' or\\\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)-1):\n",
      "    if n>-1 and n<=len(deparse)-3 and deparse[n：n+2]==\"SBAR\" or\\\n",
      "        n>0 and n<=len(deparse)-2 and deparse[n-1:n+1]==\"SBAR\" or\\\n",
      "        n>1 and n<=len(deparse) and deparse[n-2:n]=='SBAR':\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)-1):\n",
      "    if n>-1 and n<=len(deparse)-3 and deparse[n:n+2]==\"SBAR\" or\\\n",
      "        n>0 and n<=len(deparse)-2 and deparse[n-1:n+1]==\"SBAR\" or\\\n",
      "        n>1 and n<=len(deparse) and deparse[n-2:n]=='SBAR':\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)-1):\n",
      "    if n>-1 and n<=len(deparse)-4 and deparse[n:n+3]==\"SBAR\" or\\\n",
      "        n>0 and n<=len(deparse)-3 and deparse[n-1:n+2]==\"SBAR\" or\\\n",
      "        n>1 and n<=len(deparse)-1 and deparse[n-2:n+1]=='SBAR'or\\\n",
      "        n>2 and n<=len(deparse) and deparse[n-3:n]=='SBAR':\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)-1):\n",
      "    if n>=0 and n<=len(deparse)-4 and deparse[n:n+3]==\"SBAR\" or\\\n",
      "        n>=1 and n<=len(deparse)-3 and deparse[n-1:n+2]==\"SBAR\" or\\\n",
      "        n>=2 and n<=len(deparse)-1 and deparse[n-2:n+1]=='SBAR'or\\\n",
      "        n>=3 and n<=len(deparse) and deparse[n-3:n]=='SBAR':\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)):\n",
      "    if n>=0 and n<=len(deparse)-4 and deparse[n:n+3]==\"SBAR\" or\\\n",
      "        n>=1 and n<=len(deparse)-3 and deparse[n-1:n+2]==\"SBAR\" or\\\n",
      "        n>=2 and n<=len(deparse)-1 and deparse[n-2:n+1]=='SBAR'or\\\n",
      "        n>=3 and n<=len(deparse) and deparse[n-3:n]=='SBAR':\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)):\n",
      "    if n>=0 and n<=len(deparse)-4 and deparse[n:n+3]==\"SBAR\" or\\\n",
      "        n>=1 and n<=len(deparse)-3 and deparse[n-1:n+2]==\"SBAR\" or\\\n",
      "        n>=2 and n<=len(deparse)-1 and deparse[n-2:n+1]=='SBAR'or\\\n",
      "        n>=3 and n<=len(deparse) and deparse[n-3:n]=='SBAR':\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "outcome\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)):\n",
      "    if n>=0 and n<=len(deparse)-4 and deparse[n:n+3]==\"SBAR\" or\\\n",
      "        n>=1 and n<=len(deparse)-3 and deparse[n-1:n+2]==\"SBAR\" or\\\n",
      "        n>=2 and n<=len(deparse)-1 and deparse[n-2:n+1]=='SBAR'or\\\n",
      "        n>=3 and n<=len(deparse) and deparse[n-3:n]=='SBAR':\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "outcome\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n]in('(S#@', '(SBAR#@'):\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=deparse[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] in ('(S#@', '(SBAR#@'):\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=deparse[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] in ('(S#@', '(SBAR#@'):\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] =='(S#@':\n",
      "        outcome+=' '\n",
      "    elif data[n]=='(SBAR#@'):\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] =='(S#@':\n",
      "        outcome+=' '\n",
      "    elif data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] =='(S#@':\n",
      "        outcome+=' '\n",
      "    if data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] =='(S#@':\n",
      "        outcome+='('\n",
      "    if data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] =='(S#@':\n",
      "        outcome+=''\n",
      "    if data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] =='(S#@':\n",
      "        outcome+=''\n",
      "    if data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] =='(S#@':\n",
      "        outcome+=''\n",
      "    if data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n]!= :'(S#@':\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] =='(S#@':\n",
      "        outcome+=''\n",
      "    if data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n]!= '(S#@':\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] =='(S#@':\n",
      "        outcome+='('\n",
      "    elif data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] in ('(S#@','(VP','(SBAR#@'):\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n] in ('(S#@','(SBAR#@'):\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n]:\n",
      "        outcome+=\"( \"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n] !='(VP':\n",
      "        outcome+=\"(\"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n] !='(VP':\n",
      "        outcome+=\"(\"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n]!='(S#@':\n",
      "        outcome+=\"(\"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n] in('(SBAR#@','(VP'):\n",
      "        outcome+=\"( \"\n",
      "    \n",
      "    elif \")\" not in data[n] and data[n]!='(S#@':\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "print(nlp.parse(readsent(file)[5]))\n",
      "print(nlp.parse(readsent(file)[6]))\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n] in('(SBAR#@','(VP'):\n",
      "        outcome+=\"( \"\n",
      "    \n",
      "    elif \")\" not in data[n] and data[n]!='(S#@':\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "tree=Tree.fromstring(deparse)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n] in('(SBAR#@','(VP'):\n",
      "        outcome+=\"( \"\n",
      "    \n",
      "    elif \")\" not in data[n] and data[n]!='(S#@':\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n] in('(SBAR#@','(VP'):\n",
      "        outcome+=\"( \"\n",
      "    \n",
      "    elif data[n]!='(S#@':\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n] in('(SBAR#@','(VP'):\n",
      "        outcome+=\"( \"\n",
      "    \n",
      "    elif \")\" not in data[n] and data[n]!='(S#@':\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n] in('(SBAR#@','(VP'):\n",
      "        outcome+=\"( \"\n",
      "    \n",
      "    elif \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    elif \")\" not in data[n] and data[n]=='VP' and data[n+1][:3]=='(TO':\n",
      "        outcome+=\"(\"\n",
      "    elif \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    elif \")\" not in data[n] and data[n]=='VP' and data[n+1][:3]!='(TO':\n",
      "        outcome+=\"( \"\n",
      "    elif \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    elif \")\" not in data[n] and data[n]=='VP' and data[n+1]!='(TO':\n",
      "        outcome+=\"( \"\n",
      "    elif \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    if \")\" not in data[n] and data[n]=='VP' and data[n+1]!='(TO':\n",
      "        outcome+=\"( \"\n",
      "    elif \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n]=='(VP' and data[n+1]!='(TO':\n",
      "        outcome+=\"( \"\n",
      "    elif \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n]=='(VP' and data[n+1]!='(TO':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n]==\"(S#@\":\n",
      "        outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if \")\" not in data[n] and data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n]=='(VP' and data[n+1]!='(TO':\n",
      "        outcome+=\"( \"\n",
      "    #elif data[n]==\"(S#@\":\n",
      "    #    outcome+=\"(\"\n",
      "    \n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n]=='(VP' and data[n+1]!='(TO':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n] in(\"(ADJP\",\"(ADVP\",'(NP','(ROOT'):\n",
      "        outcome+=data[n]+' '\n",
      "    elif \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n]=='(VP' and data[n+1]!='(TO':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n] in(\"(ADJP\",\"(ADVP\",'(NP','(ROOT#@'):\n",
      "        outcome+=data[n]+' '\n",
      "    elif \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "def fdsyncomT4(s): #完成！！\n",
      "    CN=[] #子句数量\n",
      "    SBARS=[] #依存从句数量\n",
      "    RS=[] #sum of root 句子数量\n",
      "    VPS=[]\n",
      "    TN=[] #T-unit 结构 \n",
      "    CTN=[] #复杂T-unit 结构 \n",
      "    target=find_clause_componentT(s) # 由[(['ADVP', 'RB', 'First'], ('FRAG', 1)) ,....]样式的元组构成的列表\n",
      "    \n",
      "    for n in range(0,len(target)):\n",
      "        # target[n] 的样式 为 (['ADVP', 'RB', 'First'], ('FRAG', 1)) \n",
      "        if target[n][0][0]=='ROOT':\n",
      "            RS.append(target[n])\n",
      "            for n1 in range(n+1,len(target)-1):\n",
      "                if target[n1][0][0]=='FRAG' and target[n1][1][1]==n:\n",
      "                    if 'VP' in target[n][0]:\n",
      "                        TN.append(target[n1])  \n",
      "                        #统计T-UNIT FRAG>ROOT  root里有vp算Tunit\n",
      "                        for n2 in range(1,len(target[n1][0])): #查看是否有复杂从句归属\n",
      "                            if target[n1][0][n2]=='SBAR': #是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    CTN.append(target[n1])\n",
      "                    else:\n",
      "                        CN.append(target[n1])  #统计从句 FRAG>ROOT !<< VP 这里有个疑问 root里没vp算clause\n",
      "                    \n",
      "        #统计从句  #“S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)”\n",
      "        if target[n][0][0] in ('S','SINV','SQ') and 'VP' in target[n][0][1:]:\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][0][0]=='VP' and target[n1][0][1] in ('VBD','VBP','VBZ','MD'):\n",
      "                    if target[n1][1][1]==n: #VP直接从属于 S \n",
      "                        if target[n] not in CN:\n",
      "                            CN.append(target[n])  #VP下面直接存在 VBD VBP VBZ 和 MD \n",
      "\n",
      "                    else:                  #VP间接从属于 S不能计算入内\n",
      "                        for n3 in range(1,n1-1): \n",
      "                            if target[n1][1][1]==n3 and target[n3][1][1]==n:\n",
      "                                n2=1\n",
      "                                while n2<=len(target[n1][0])-1 and target[n1][0][n2] not in ('VBD','VBP','VBZ','MD'):\n",
      "                                    n2+=1\n",
      "                                if n2<len(target[n1][0]):  #保证VP下面有VBD,VBP,VBZ,MD 算法\n",
      "                                    if target[n] not in CN:\n",
      "                                        CN=CN\n",
      "                                        #CN.append(target[n])\n",
      "                                \n",
      "        \n",
      "        # 统计依存从句 SBAR<(S|SINV|SQ<(VP<#MD|VBD|VBP|VBZ)) \n",
      "        if target[n][0][0] =='SBAR':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SINV','SQ') and 'VP' in target[n1][0][1:]:      \n",
      "                        for n2 in range(n1+1,len(target)):\n",
      "                            if target[n2][0][0]=='VP' and target[n2][0][1] in ('VBD','VBP','VBZ','MD'): #VP直接从属于 S \n",
      "                                    if target[n] not in SBARS:\n",
      "                                        SBARS.append(target[n])\n",
      "                                #不考虑VP间接从属于 S\n",
      "\n",
      "                                        \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ>ROOT  直接从属于root\n",
      "        if target[n][0][0]=='ROOT':\n",
      "            for n1 in range(n+1,len(target)):\n",
      "                if target[n1][1][1]==n: \n",
      "                    if target[n1][0][0] in ('S','SBARQ','SINV','SQ'):\n",
      "                        if target[n1] not in TN:\n",
      "                            TN.append(target[n1])\n",
      "                        # 统计 Complex T-UNIT S|SBARQ|SINV|SQ [> ROOT] << (SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)))\n",
      "                        for n2 in range(1,len(target[n1][0])):\n",
      "                            if target[n1][0][n2]=='SBAR':  #查看是否有复杂从句归属\n",
      "                                n3=n2+1\n",
      "                                while n3<=len(target[n1][0])-1 and target[n1][0][n3] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n3+=1\n",
      "                                if n3<len(target[n1][0]):\n",
      "                                    if target[n1] not in CTN:\n",
      "                                        CTN.append(target[n1])\n",
      "                                    \n",
      "                            \n",
      "        # 统计T-UNIT : S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP] 与 从句并列且不属于SBAR或VP\n",
      "        if target[n][0][0] in ('S','SBARQ','SINV','SQ') and n-1>0:\n",
      "            for n1 in range(0,n-1): #target[n1] 查看并列情况\n",
      "                if target[n1][1]==target[n][1] and target[n1][0][0] in ('S','SBARQ','SINV','SQ'): \n",
      "                    tempn=0\n",
      "                    for n2 in range(0,len(target)-1): #target[n2] 查看 是否从属于 从句结构\n",
      "                        if tempn==0 and target[n2][0][0] in ('VP','SBAR'):\n",
      "                            if len(target[n2][0])>=len(target[n][0]) and match(target[n2][0],target[n][0]):\n",
      "                        #如果前面计数是0 且 存在并列结构\n",
      "                                tempn+=1\n",
      "                    if tempn==0:\n",
      "                        TN.append(target[n])\n",
      "                    # 统计Complex T_Unit:\"S|SBARQ|SINV|SQ[$-- S|SBARQ|SINV|SQ !>> SBAR|VP]] <<(SBAR<(S|SQ|SINV<(VP<#MD|VBP|VBZ|VBD)\"\n",
      "                        for n3 in range(1,len(target[n][0])): #查看是否有复杂从句归属\n",
      "                            if target[n][0][n3]=='SBAR': #是否有复杂从句归属\n",
      "                                n4=n3+1\n",
      "                                while n4<=len(target[n][0])-1 and target[n][0][n4] not in ('MD','VBP','VBZ','VBD'):\n",
      "                                    n4+=1\n",
      "                                if n4<len(target[n][0]):\n",
      "                                    CTN.append(target[n])\n",
      "                        \n",
      "                                        \n",
      "                                        \n",
      "\n",
      "\n",
      "############################################################至此完成        \n",
      "\n",
      "\n",
      "    \n",
      "    CNN=Ncnp(s)\n",
      "    VN=Nverb(s)\n",
      "    print('\\n\\n')\n",
      "    print(\"*\"*50,'COMPONENTS','*'*50)\n",
      "    print('1. SENTENCE NUM:','\\t',RS)\n",
      "    print('2. CLAUSE NUM:','\\t',CN)\n",
      "    print('3. T_UNIT NUM:','\\t',TN)\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',SBARS)\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',CTN)\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    print('\\n\\n')\n",
      "    print(s)\n",
      "    print(nlp.parse(s))\n",
      "    print('*'*50,'D-----A-----T-----A','*'*50)\n",
      "    print('\\n\\n')\n",
      "    print('1. SENTENCE NUM:','\\t',len(RS))\n",
      "    print('2. CLAUSE NUM:','\\t',len(CN))\n",
      "    print('3. T_UNIT NUM:','\\t',len(TN))\n",
      "    print('4. DEPENDENT CLAUSE NUM:','\\t',len(SBARS))\n",
      "    print('5. COMPLEX T_UNIT NUM:','\\t',len(CTN))\n",
      "    print('6. COMPLEX NOUN PHRASE NUM:','\\t',CNN)\n",
      "    print('7. VERB PHRASE NUM:','\\t',VN)\n",
      "    return(len(RS),len(CN),len(TN),len(SBARS),len(CTN),CNN,VN)\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "SenComAnalyser5(file)\n",
      "fdsyncomT4(file)\n",
      "def SenComAnalyser5(file):\n",
      "    target=readsent(file)\n",
      "    data=[]\n",
      "    sumWN=[]\n",
      "    result=''\n",
      "    datatitle=[\"W\",\"W/T\",\"W/C\",\"C\",\"C/T\",\"CT/T\",\"DC/C\",\"DC/T\",\"T\",'CN',\"CN/C\",\"CN/T\",\"VP/C\"]\n",
      "    headline='\\t长度特征 \\t \\t \\t子句数量 \\t从属情况 \\t \\t \\t \\t并列情况 \\t特殊结构占比\\t \\t \\t'\n",
      "    result+=headline+'\\n'\n",
      "    headline='SENT\\COMP'\n",
      "    for t in datatitle:\n",
      "        headline+='\\t'+t\n",
      "    result+=headline+'\\n'\n",
      "    print(headline)\n",
      "    n=1\n",
      "    W=0\n",
      "    T=0\n",
      "    C=0\n",
      "    DC=0\n",
      "    CT=0\n",
      "    CN=0\n",
      "    VP=0\n",
      "    for line in target:\n",
      "        w=len(nltk.word_tokenize(line))\n",
      "        l=''\n",
      "        l+='sentence'+str(n)+'\\t'\n",
      "        l+=str(w)+'\\t'   #W\n",
      "        W+=w\n",
      "        C+=data[1]\n",
      "        T+=data[2]\n",
      "        DC+=data[3]\n",
      "        CT+=data[4]\n",
      "        CN+=data[5]\n",
      "        VP+=data[6]\n",
      "        data=fdsyncomT4(line)\n",
      "        if data[2] !=0 :       #W/T\n",
      "            l+=str(round(w/data[2],2))  +'\\t' \n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :  #W/C\n",
      "            l+=str(round(w/data[1],2))  +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[1])+'\\t'   #C\n",
      "        \n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[1]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[4]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[3]/data[1],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[3]/data[2],2))   +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[2])+'\\t'\n",
      "        l+=str(data[5])+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[5]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[5]/data[2],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[6]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        result+=l+'\\n'\n",
      "        n+=1\n",
      "        #print('sentence'+str(n),'\\t',l)\n",
      "    print(result)\n",
      "    return(W,T,C,DC,CT,CN,PV)\n",
      "fdsyncomT3(readsent(file)[6])\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "SenComAnalyser5(file)\n",
      "def SenComAnalyser5(file):\n",
      "    target=readsent(file)\n",
      "    data=[]\n",
      "    sumWN=[]\n",
      "    result=''\n",
      "    datatitle=[\"W\",\"W/T\",\"W/C\",\"C\",\"C/T\",\"CT/T\",\"DC/C\",\"DC/T\",\"T\",'CN',\"CN/C\",\"CN/T\",\"VP/C\"]\n",
      "    headline='\\t长度特征 \\t \\t \\t子句数量 \\t从属情况 \\t \\t \\t \\t并列情况 \\t特殊结构占比\\t \\t \\t'\n",
      "    result+=headline+'\\n'\n",
      "    headline='SENT\\COMP'\n",
      "    for t in datatitle:\n",
      "        headline+='\\t'+t\n",
      "    result+=headline+'\\n'\n",
      "    print(headline)\n",
      "    n=1\n",
      "    W=0\n",
      "    T=0\n",
      "    C=0\n",
      "    DC=0\n",
      "    CT=0\n",
      "    CN=0\n",
      "    VP=0\n",
      "    for line in target:\n",
      "        w=len(nltk.word_tokenize(line))\n",
      "        l=''\n",
      "        l+='sentence'+str(n)+'\\t'\n",
      "        l+=str(w)+'\\t'   #W\n",
      "        W+=w\n",
      "        data=fdsyncomT4(line)\n",
      "        C+=data[1]\n",
      "        T+=data[2]\n",
      "        DC+=data[3]\n",
      "        CT+=data[4]\n",
      "        CN+=data[5]\n",
      "        VP+=data[6]\n",
      "        if data[2] !=0 :       #W/T\n",
      "            l+=str(round(w/data[2],2))  +'\\t' \n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :  #W/C\n",
      "            l+=str(round(w/data[1],2))  +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[1])+'\\t'   #C\n",
      "        \n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[1]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[4]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[3]/data[1],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[3]/data[2],2))   +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[2])+'\\t'\n",
      "        l+=str(data[5])+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[5]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[5]/data[2],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[6]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        result+=l+'\\n'\n",
      "        n+=1\n",
      "        #print('sentence'+str(n),'\\t',l)\n",
      "    print(result)\n",
      "    return(W,T,C,DC,CT,CN,PV)\n",
      "fdsyncomT3(readsent(file)[6])\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "SenComAnalyser5(file)\n",
      "def SenComAnalyser5(file):\n",
      "    target=readsent(file)\n",
      "    data=[]\n",
      "    sumWN=[]\n",
      "    result=''\n",
      "    datatitle=[\"W\",\"W/T\",\"W/C\",\"C\",\"C/T\",\"CT/T\",\"DC/C\",\"DC/T\",\"T\",'CN',\"CN/C\",\"CN/T\",\"VP/C\"]\n",
      "    headline='\\t长度特征 \\t \\t \\t子句数量 \\t从属情况 \\t \\t \\t \\t并列情况 \\t特殊结构占比\\t \\t \\t'\n",
      "    result+=headline+'\\n'\n",
      "    headline='SENT\\COMP'\n",
      "    for t in datatitle:\n",
      "        headline+='\\t'+t\n",
      "    result+=headline+'\\n'\n",
      "    print(headline)\n",
      "    n=1\n",
      "    W=0\n",
      "    T=0\n",
      "    C=0\n",
      "    DC=0\n",
      "    CT=0\n",
      "    CN=0\n",
      "    VP=0\n",
      "    for line in target:\n",
      "        w=len(nltk.word_tokenize(line))\n",
      "        l=''\n",
      "        l+='sentence'+str(n)+'\\t'\n",
      "        l+=str(w)+'\\t'   #W\n",
      "        W+=w\n",
      "        data=fdsyncomT4(line)\n",
      "        C+=data[1]\n",
      "        T+=data[2]\n",
      "        DC+=data[3]\n",
      "        CT+=data[4]\n",
      "        CN+=data[5]\n",
      "        VP+=data[6]\n",
      "        if data[2] !=0 :       #W/T\n",
      "            l+=str(round(w/data[2],2))  +'\\t' \n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :  #W/C\n",
      "            l+=str(round(w/data[1],2))  +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[1])+'\\t'   #C\n",
      "        \n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[1]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[4]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[3]/data[1],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[3]/data[2],2))   +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[2])+'\\t'\n",
      "        l+=str(data[5])+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[5]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[5]/data[2],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[6]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        result+=l+'\\n'\n",
      "        n+=1\n",
      "        #print('sentence'+str(n),'\\t',l)\n",
      "    print(result)\n",
      "    return(W,T,C,DC,CT,CN,VP)\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "SenComAnalyser5(file)\n",
      "def SenComAnalyser5(file):\n",
      "    target=readsent(file)\n",
      "    data=[]\n",
      "    sumWN=[]\n",
      "    result=''\n",
      "    datatitle=[\"W\",\"W/T\",\"W/C\",\"C\",\"C/T\",\"CT/T\",\"DC/C\",\"DC/T\",\"T\",'CN',\"CN/C\",\"CN/T\",\"VP/C\"]\n",
      "    headline='\\t长度特征 \\t \\t \\t子句数量 \\t从属情况 \\t \\t \\t \\t并列情况 \\t特殊结构占比\\t \\t \\t'\n",
      "    result+=headline+'\\n'\n",
      "    headline='SENT\\COMP'\n",
      "    for t in datatitle:\n",
      "        headline+='\\t'+t\n",
      "    result+=headline+'\\n'\n",
      "    print(headline)\n",
      "    n=1\n",
      "    W=0\n",
      "    T=0\n",
      "    C=0\n",
      "    DC=0\n",
      "    CT=0\n",
      "    CN=0\n",
      "    VP=0\n",
      "    for line in target:\n",
      "        w=len(nltk.word_tokenize(line))\n",
      "        l=''\n",
      "        l+='sentence'+str(n)+'\\t'\n",
      "        l+=str(w)+'\\t'   #W\n",
      "        W+=w\n",
      "        data=fdsyncomT4(line)\n",
      "        C+=data[1]\n",
      "        T+=data[2]\n",
      "        DC+=data[3]\n",
      "        CT+=data[4]\n",
      "        CN+=data[5]\n",
      "        VP+=data[6]\n",
      "        if data[2] !=0 :       #W/T\n",
      "            l+=str(round(w/data[2],2))  +'\\t' \n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :  #W/C\n",
      "            l+=str(round(w/data[1],2))  +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[1])+'\\t'   #C\n",
      "        \n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[1]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[4]/data[2],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[3]/data[1],2))    +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[3]/data[2],2))   +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        l+=str(data[2])+'\\t'\n",
      "        l+=str(data[5])+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[5]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[2] !=0 :\n",
      "            l+= str(round(data[5]/data[2],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        if data[1] !=0 :\n",
      "            l+= str(round(data[6]/data[1],2)) +'\\t'\n",
      "        else:\n",
      "            l+='0'+'\\t'\n",
      "        result+=l+'\\n'\n",
      "        n+=1\n",
      "        #print('sentence'+str(n),'\\t',l)\n",
      "    print(result)\n",
      "    dataresult=(W,round(W/T,2),round(W/C,2),round(C/T,2),round(CT/T,2),round(DC/C,2),round(DC/T,2),T,CN,round(CN/C,2),round(CN/T,2),round(VP/C,2))\n",
      "    return((W,T,C,DC,CT,CN,VP),(dataresult))\n",
      "file=r'C:\\Users\\spike\\Documents\\论文相关\\Stanford NLP句法教学\\b1u1.txt'\n",
      "SenComAnalyser5(file)\n",
      "from nltk.tree import Tree #将句子改为名词短语、形容词短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(deparse)):\n",
      "    if n>=0 and n<=len(deparse)-4 and deparse[n:n+3]==\"SBAR\" or\\\n",
      "        n>=1 and n<=len(deparse)-3 and deparse[n-1:n+2]==\"SBAR\" or\\\n",
      "        n>=2 and n<=len(deparse)-1 and deparse[n-2:n+1]=='SBAR'or\\\n",
      "        n>=3 and n<=len(deparse) and deparse[n-3:n]=='SBAR':\n",
      "        outcome+=\" \"\n",
      "    else:\n",
      "        outcome+=deparse[n]\n",
      "outcome\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "from nltk.tree import Tree #将句子改为短语填图练习\n",
      "#s=text\n",
      "outcome=\"\"\n",
      "deparse= nlp.parse(readsent(file)[6])\n",
      "deparse=deparse.replace('\\r',\"#\").replace('\\n',\"@\")\n",
      "data=deparse.split()\n",
      "print(data)\n",
      "num=0\n",
      "for n in range(0,len(data)):\n",
      "    if data[n]=='(SBAR#@':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n]=='(VP' and data[n+1]!='(TO':\n",
      "        outcome+=\"( \"\n",
      "    elif data[n] in(\"(ADJP\",\"(ADVP\",'(NP','(ROOT#@'):\n",
      "        outcome+=data[n]+' '\n",
      "    elif \")\" not in data[n]:\n",
      "        outcome+=\"(\"\n",
      "    else:\n",
      "        outcome+=data[n]+' '\n",
      "        \n",
      "print(outcome)\n",
      "\n",
      "outcome=outcome.replace('#','\\r').replace('@','\\n')\n",
      "print(outcome)\n",
      "outcome2=''\n",
      "no=1\n",
      "for n in range(0,len(outcome)):\n",
      "    if n-1>=0 and outcome[n-1]==\"(\" and outcome[n]==' ':\n",
      "        outcome2+=str(no)+'____'\n",
      "        no+=1\n",
      "    else:\n",
      "        outcome2+=outcome[n]\n",
      "tree=Tree.fromstring(outcome2)\n",
      "tree.draw()\n",
      "history\n"
     ]
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
